{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/opt/deep_Learning/utilities\")\n",
    "from spark_setup import SetupEnvironment \n",
    "environment = SetupEnvironment(\n",
    "                conda_env = \"my_env\", # should match with jupyter kernel used (e.g. my_env or deep_learning)\n",
    "                project_name = \"NoLivyExampleProject\", # defaults to Domino project name if not set\n",
    "                additional_settings = {\n",
    "\n",
    "                    #\"spark.executor_memory\": \"130G\" ,\n",
    "                    \"spark.driver.memory\": \"130G\" ,\n",
    "                    #\"spark.executor.cores\": \"2\",\n",
    "                    #cores max == 24 instances * 20 each(large) * 0.8 = rough384\n",
    "                    #\"spark.cores.max\": \"180\",\n",
    "                    #\"spark.driver.memoryOverhead\": \"20G\", \n",
    "                    #\"spark.executor_memoryOverhead\" : \"20G\" ,\n",
    "                    #\"spark.sql.autoBroadcastJoinThreshold\" : '-1', \n",
    "                    #\"spark.sql.shuffle.partitions\"  : \"200\",\n",
    "                    #\"spark.shuffle.service.enabled\" : 'true',\n",
    "                    #'spark.excludeOnFailure.enabled' : \"true\",\n",
    "                    #\"spark.excludeOnFailule.application.fetchFailure.enabled\":'true' ,\n",
    "                    #\"spark.executorEnv_PYARROW_IGNORE_TIMEZONE\":\"1\" ,\n",
    "                    #\"spark.sql.execution.arrow.pyspark.enabled\" : 'true',\n",
    "                    #\"spark.sql.execution.arrow.pyspark.fallback.enabled\":'true',\n",
    "                } # additional spark configuration (optional)\n",
    ")\n",
    "\n",
    "spark = environment.setup_spark()\n",
    "dcRead = environment.setup_DataCatalog()\n",
    "s3 = environment.setup_s3()\n",
    "\n",
    "#'spark.files.fetchTimeout' : '300s' \n",
    "#'spark.network.timeout' : '360s'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1af28c",
   "metadata": {},
   "source": [
    "### Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cf8056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs \n",
    "import pickle \n",
    "import pyarrow \n",
    "import os \n",
    "import sys\n",
    "import pandas as pd \n",
    "import datetime \n",
    "import numpy as np\n",
    "from pyarrow import parquet as pa \n",
    "import json\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tensorflow import keras \n",
    "from tensorflow. keras import layers \n",
    "from matplotlib import pyplot as plt \n",
    "import pyspark.sql.functions as F\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92829fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vizualization modules\n",
    "\n",
    "from plotly.offline import download_plotlyis, plot, iplot \n",
    "import plotly.graph_objs as go \n",
    "import plotly.io as pio \n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e04e42",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e0987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pipeline_parquet(s3a_bucket_str, dataframe_name, date_start, date_end=None, verbose=False, version=None):\n",
    "\n",
    "    #writes loaded data into parquet for future consumption \n",
    "    \n",
    "    if date_start == None:\n",
    "        file_path = s3a_bucket_str+dataframe_name \n",
    "    elif date_end != None:\n",
    "        file_path = s3a_bucket_str+dataframe_name+'_{}'.format(date_start)+'_{}'.format(date_end)\n",
    "    else:\n",
    "        file_path = s3a_bucket_str+dataframe_name+'_{}'.format(date_start)\n",
    "\n",
    "    if verbose==True:\n",
    "        print(f'Reading from {file_patht}')\n",
    "    else:\n",
    "        pass\n",
    "    if version == None:\n",
    "        pass\n",
    "    else:\n",
    "        file_path =file_path +'_v'+str(version)\n",
    "    new_file = spark.read.parquet(file_path)\n",
    "    return new_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d886c8",
   "metadata": {},
   "source": [
    "### Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd95d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Reading/Writing\n",
    "PROBE_DATE = '2022-01-20'\n",
    "CURRENT_VERSION = 0\n",
    "BUCKET_STRING = \"s3a://9262-4618-2427-mrp-cat-kms/users/k30577/\"\n",
    "RANDOM_SEED = 42\n",
    "TRAIN_PROPORTION = 0.75\n",
    "#SLICES = 6 #(chosen in last step of process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420e668",
   "metadata": {},
   "source": [
    "### Read in Sampled IV Chain Data from 2.5\n",
    "\n",
    "Run Code from one of next two blocks, Sampling Method 1 contains exactly 1 subwindow from each chain, whereas Sampling Method 2 contains a 0.5% sample of all subwindows. Each is similar in overall size. SM1 was used for retrains 1,2. Where as SM2 was used for retrain 3, Sampling method 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling Method 1\n",
    "\n",
    "Final_IV_Data_Sample = read_pipeline_parquet(BUCKET_STRING, \"actrain9_IV_normalized_rows_030_1ea\",PROBE_DATE, version = CURRENT_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_IV_Data_Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cde21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling Method 2\n",
    "\n",
    "Final_IV_Data_Sample = read_pipeline_parquet(BUCKET_STRING,\"actrain9_IV_normalized_rows_005\", PROBE_DATE, version=CURRENT_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe624b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_IV_Data_Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data to pandas format\n",
    "Final_IV_Data_Sample_Pre_PD = Final_IV_Data_Sample.select('chain_id', 'all_items')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478bfe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_IV_Data_Sample_PD = Final_IV_Data_Sample_Pre_PD.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0236ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_IV_Data_Sample_PD.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce86980",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Final_IV_Data_Sample_PD.isna().sum()[Final_IV_Data_Sample_PD.isna().sum() > 0], columns = ['count nan values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_IV_Data_Sample_PD.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec8929",
   "metadata": {},
   "source": [
    "#### Get Rid of NaN rows and split and into training and validation (no test for unsupervised data) (Split By Chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_IV_Data_Sample_PD['np_items'] = Final_IV_Data_Sample_PD['all_items'J.to_numpy()\n",
    "Final_IV_Data_Sample_PD['np_items_1st'] = Final_IV_Data_Sample_PD['np_items'].str[0]\n",
    "Final_IV_Data_Sample_PD_DeNan= Final_IV_Data_Sample_PD[Final_IV_Data_Sample_PD['np_items_1st'].isna()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_IV_Data_Sample_PD_DeNan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Final_IV_Data_Sample_PD_DeNan.isna().sum()[Final_IV_Data_Sample_PD_DeNan.isna().sum() > 0], columns = ['count nan values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861700ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_IV_Data_Sample_PD_DeNan.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806c7372",
   "metadata": {},
   "source": [
    "### Split into Training and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5280e24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_events = len(Final_IV_Data_Sample_PD_DeNan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb6abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_events = Len(Final_IV_Data_002_Sample_PD_DeNan)\n",
    "train_val_df = Final_IV_Data_Sample_PD_DeNan.sample(frac=1, random_state = RANDOM_SEED).reset_index()\n",
    "train_df = train_val_df.loc[: int(TRAIN_PROPOTION*num_events)]\n",
    "val_df = train_val_df.loc[int(TRAIN_PROPOTION*num_events)+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6bab47",
   "metadata": {},
   "source": [
    "### Create and Reshape Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.stack(train_df['all_items'].to_numpy(), axis=0)\n",
    "X_train = X_train.reshape(*(list(X_train.shape)+[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c74f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca15ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.stack(val_df['all_items'].to_numpy(), axis=0)\n",
    "X_val = X_val.reshape(*(list(X_val.shape)+[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d371542",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb3e65",
   "metadata": {},
   "source": [
    "### Build Model Architecture\n",
    "\n",
    "Modeling Decisions:\n",
    "\n",
    "The original autoencoder artifact had 64 neurons, informing the NUM_NEURONS decision.\n",
    "\n",
    "The loss function was changed from 'mae' to 'mse' between retraining efforts, the latter decision brought the mae scores for the validatio\n",
    "\n",
    "line with the scores given by the artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a09457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed\n",
    "\n",
    "DROPOUT = 0.2\n",
    "NUM_NEURONS = 64 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(NUM_NEURONS, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "modeL.add(Dropout(rate=DROPOUT))\n",
    "model.add(RepeatVector(X_train.shape[1]))\n",
    "model.add(LSTM(NUM_NEURONS,return_sequences=True)) \n",
    "model.add(Dropout(rate=DROPOUT)) \n",
    "model.add(TimeDistributed(Dense(X_train.shape[2]))) \n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f54cfd",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ts = datetime.datetime.now()\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 2048*2\n",
    "\n",
    "history = model.fit(\n",
    "\n",
    "    X_train, \n",
    "    X_train, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    validation_data=(X_val,X_val), \n",
    "    callbacks=[\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, mode =\"min\")\n",
    "            ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28730b40",
   "metadata": {},
   "source": [
    "### validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb397efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.style.use('dark_background')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e886e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], c = 'b', label='train')\n",
    "plt.plot(history.history['val_loss'], c = 'y', label='validation')\n",
    "plt.legend()\n",
    "plt.title(\"Reconstruction Loss \")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff9c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pred = model.predict(X_train)\n",
    "x_val_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58473927",
   "metadata": {},
   "outputs": [],
   "source": [
    "mets = []\n",
    "train = []\n",
    "val = []\n",
    "\n",
    "mets.append('mean_squared_error')\n",
    "train.append(mean_squared_error(x_train, x_train_pred))\n",
    "val.append(mean_squared_error(x_val, x_val_pred))\n",
    "\n",
    "mets.append('mean_absolute_error')\n",
    "train.append(mean_absolute_error(x_train, x_train_pred))\n",
    "val.append(mean_absolute_error(x_val, x_val_pred))\n",
    "\n",
    "mets.append('r2_multi_output_regres')\n",
    "train.append(MultiOutputRegressor(Ridge(random_state=123)).fit(x_train, x_train_pred).score(x_train, x_train_pred))\n",
    "val.append(MultiOutputRegressor(Ridge(random_state=123)).fit(x_val, x_val_pred).score(x_val, x_val_pred))\n",
    "\n",
    "mets.append('r2_sklearn')\n",
    "train.append(r2_score(x_train, x_train_pred))\n",
    "val.append(r2_score(x_val, x_val_pred))\n",
    "\n",
    "metrics = pd.DataFrame({'metrics' : mets, 'train_set' : train, 'validation_set' : val })\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ce4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [x_train.tolist(), x_train_pred.tolist()]\n",
    "chi_square_dict = {'stats':chi2_contingency(data)[0], 'p':chi2_contingency(data)[1],'dof':chi2_contingency(data)[2]}\n",
    "print(f'train_chi2_contingency: \\n {chi_square_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [x_val.tolist(), x_val_pred.tolist()]\n",
    "chi_square_dict = {'stats':chi2_contingency(data)[0], 'p':chi2_contingency(data)[1],'dof':chi2_contingency(data)[2]}\n",
    "print(f'validation_chi2_contingency: \\n {chi_square_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a38352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c69c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d4e1c8f",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "autoencoder_retrain_0 - 256 Neurons, MAE, Sampling Method 1 (Not placed in final artifacts folder considered in model selection, different number of parameters.)\n",
    "\n",
    "autoencoder_retrain_1 - 64 Neurons, MAE, Sampling Method 1\n",
    "\n",
    "*autoencoder_retrain_2 - 64 Neurons, MSE, Sampling Method 1 (Current Best Model)\n",
    "\n",
    "autoencoder_retrain_3 - 64 Neurons, MSE, Sampling Method 2\n",
    "\n",
    "model_mse. save(\"autoencoder_retrain_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dae4dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"autoencoder_retrain_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d2f1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
