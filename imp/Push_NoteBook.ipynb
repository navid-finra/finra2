{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import mrmr\n",
    "import time\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, IntegerType, DoubleType \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2,VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "from boruta import BorutaPy\n",
    "from random import sample\n",
    "from functools import partial\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/deep learning/utilities')\n",
    "\n",
    "from spark_setup import SetupEnvironment\n",
    "environment = SetupEnvironment (conda_env='my_env')\n",
    "spark = environment.setup_spark()\n",
    "dcRead = environment.setup_DataCatalog()\n",
    "s3 = environment.setup_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c836e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eae493",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/repos/mme010/DS_Artifacts/prototypes/V2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3186cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from push_training.common import mrp_pipeline , preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANCE_THRESHOLD = 0.0001\n",
    "FLOATING_POINT_ERROR = 0.000001\n",
    "VIF_THRESHOLD = 10\n",
    "CORRELATION_THRESHOLD = 0.95 \n",
    "CORRELATION_METHOD = \"pearson\"\n",
    "MAX_EVALS_TUNE_TRAIN = 300\n",
    "N_FEATURES_TO_TRY = np.round(np.geomspace (10, 500, 16)).astype(int)\n",
    "MAX_EVALS = 50\n",
    "FEATURE_SELECTION_METHOD = \"modified_mrmr\" #boruta\n",
    "NON_FEATURE_COLS = ['id', 'id_secondary', 'LABEL', 'tdate', 'pttrn_trade_dt_all' , 'split']\n",
    "SOURCE_TSFRESH_DATATYPE_DICTIONARY = {'length': 'discrete', 'maximum': 'continuous', 'mean':'continuous', 'median':'continuous','minimum':'continuous','quantile':'continuous'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_put(data, name, s3):\n",
    "    with s3.open(f'{s3.project_home}/{name}', 'wb') as f: \n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def pickle_get(name, s3):\n",
    "    with s3.open(f'{s3.project_home}/{name}', 'rb') as f: \n",
    "        return pickle.load(f)\n",
    "\n",
    "def keras_put(model, name, s3): \n",
    "    tmp_path='/tmp/tmp_model.h5'\n",
    "    model.save(tmp_path)\n",
    "    s3.put(tmp_path, f'{s3.project_home}/{name}')\n",
    "\n",
    "def keras_get(name, s3): \n",
    "    tmp_path ='/tmp/tmp_model/h5'\n",
    "    s3.get(f'{s3.project_home}/{name}', tmp_path) \n",
    "    model = tf.keras.models.load_model(tmp_path)\n",
    "    return model\n",
    "\n",
    "def json_put(data, name, s3):\n",
    "    with s3.open(f'{s3.project_home}/{name}', 'w') as f: \n",
    "        json.dump(data, f)\n",
    "\n",
    "def json_get(name, s3):\n",
    "    with s3.open(f'{s3.project_home}/{name}', 'r') as f: \n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60733b9c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f52449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetTsfreshSimplifiedMapping(mrp_pipeline.MRPStep):\n",
    "\n",
    "    name = \"tsfesh_simplifed_mapping\"\n",
    "\n",
    "    def get_map_without_prepends (self, metadata):\n",
    "        \"\"\"Get mapping of tsfresh names to simplified tsfresh names\n",
    "        (without prepended string indicating source time series)\"\"\"\n",
    "\n",
    "        start = 'ASK_PR__' \n",
    "        simplified_tsfresh_map = {\n",
    "            entry['column_name'][len (start):]: entry['tsfresh_feature_name'][len (start):] \n",
    "            for entry in metadata \n",
    "            if entry['column_name'].startswith(start)\n",
    "        }\n",
    "        return simplified_tsfresh_map\n",
    "\n",
    "    def prepend_to_map(self, to_prepend, mapping):\n",
    "        \"\"\"Prepend string 'to prepend' to strings on each side of mapping\"\"\"\n",
    "        prepended_map = {\n",
    "            to_prepend+key: to_prepend+value \n",
    "            for key, value in mapping.items()\n",
    "        }\n",
    "        return prepended_map\n",
    "\n",
    "    def get_full_map(self, no_prepends_map):\n",
    "        \"\"\"Get full MTC push tsfresh names to simplified tsfresh names \n",
    "        (including prepended strings indicating source time serieses)\"\"\"\n",
    "        full_simplified_tsfresh_map = {} \n",
    "        for to_prepend in ['Price_f__', 'cumul_qty_f_prtcp_', 'PRICE_f_prtcp_']: \n",
    "            full_simplified_tsfresh_map.update(\n",
    "                self.prepend_to_map(to_prepend, no_prepends_map)\n",
    "            )\n",
    "        return full_simplified_tsfresh_map\n",
    "    \n",
    "    def main(self, mi_feature_metadata):\n",
    "        no_prepends_map = self.get_map_without_prepends(mi_feature_metadata) \n",
    "        full_simplified_tsfresh_map = self.get_full_map(no_prepends_map) \n",
    "\n",
    "        print('\\n tsfesh_simplifed_mapping > full_simplified_tsfresh_map:', full_simplified_tsfresh_map,'\\n')\n",
    "        \n",
    "        return {\n",
    "        'full_simplified_tsfresh_map': full_simplified_tsfresh_map\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f842d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tsfresh_simplified_mapping_step(dcRead, s3, spark):\n",
    "    def ts_mapping_load_inputs():\n",
    "        return {\n",
    "            'mi_feature_metadata': json.load(open('/repos/mme010/DS_Artifacts/prototypes/v2/push_training/momentum_ignition_feature_metadata.json', 'r'))\n",
    "        }\n",
    "        \n",
    "    def ts_mapping_handle_outputs(full_simplified_tsfresh_map): \n",
    "        pickle_put(\n",
    "            full_simplified_tsfresh_map, \n",
    "            'full_simplified_tsfresh_map.pickle', \n",
    "            s3)\n",
    "\n",
    "    step_ts_mapping = GetTsfreshSimplifiedMapping(\n",
    "        input_loader=ts_mapping_load_inputs, \n",
    "        output_handler=ts_mapping_handle_outputs)\n",
    "    \n",
    "    return step_ts_mapping\n",
    "\n",
    "tsfresh_simplified_mapping_step = make_tsfresh_simplified_mapping_step(dcRead, s3, spark)\n",
    "tsfresh_simplified_mapping_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('full_simplified_tsfresh_map.pickle',s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4dae1",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb9760",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMapping (mrp_pipeline.MRPStep):\n",
    "\n",
    "    name = \"get_a_tsfresh_mapping\"\n",
    "\n",
    "    def expand_json(self, df, json_col, schema, prefix='EXPANDED_'):\n",
    "        \"\"\"Expand json columns to individual columns for each feature\"\"\"\n",
    "        non_json_cols = [c for c in df.columns if F.col != json_col] \n",
    "        df = df.withColumn(\n",
    "            json_col + '_unpacked_struct', \n",
    "            F.from_json(F.col(json_col), schema)\n",
    "        )\n",
    "        new_cols_to_expand = df.schema[json_col + '_unpacked_struct'].dataType.names \n",
    "        return df.select(\n",
    "            non_json_cols \n",
    "            + [ \n",
    "                F.col(\n",
    "                    json_col + '_unpacked_struct.' + c \n",
    "                ).cast(DoubleType()).alias(prefix + c) \n",
    "                for c in new_cols_to_expand\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_a_tsfresh_mapping(self, push_events):\n",
    "        \"\"\"Get mapping from a_? column names to tsfresh column names\"\"\"\n",
    "        tsfresh_cols = list(\n",
    "            json.loads(\n",
    "                push_events.limit(1).rdd.collect()[0]['PRDCN FATRS_TX']\n",
    "            ).keys()\n",
    "        )\n",
    "        schema = StructType([\n",
    "            StructField(name, StringType())\n",
    "            for name in tsfresh_cols\n",
    "\n",
    "        ]) \n",
    "        tsfresh_data = self.expand_json(push_events, 'PRDCN_FATRS_TX', schema, '')\n",
    "        price_cols = tsfresh_cols[:779] \n",
    "        cumul_qty_prtcp_cols = tsfresh_cols[779:1558] \n",
    "        price_prtcp_cols = tsfresh_cols[1558:]\n",
    "        feats = (\n",
    "              [f'a{n}_1' for n in np.arange(779)] \n",
    "            + [f'a{n}_2' for n in np.arange(779)] \n",
    "            + [f'a{n}_3' for n in np.arange(779)]\n",
    "        )\n",
    "        a_to_tsfresh = {} \n",
    "        for feat in feats:\n",
    "            ts = feat.split('_')[-1] \n",
    "            i_col = int(feat.split('_')[0][1:])\n",
    "            assert i_col < 780, f' Column number too high {i_col},  ts:{ts}'\n",
    "            if ts == '1':\n",
    "                a_to_tsfresh[feat] = price_cols[i_col] \n",
    "            elif ts == '2':\n",
    "                a_to_tsfresh[feat] = cumul_qty_prtcp_cols[i_col] \n",
    "            elif ts == '3':\n",
    "                a_to_tsfresh[feat] = price_prtcp_cols[i_col] \n",
    "            else:\n",
    "                print(f'No time-series {ts}')\n",
    "                break \n",
    "        return a_to_tsfresh\n",
    "    \n",
    "    def main(self, mtc_prdcn_table): \n",
    "        print('\\n get_a_tsfresh_mapping > a_to_tsfresh:', self.get_a_tsfresh_mapping(mtc_prdcn_table),'\\n')\n",
    "        return {\n",
    "            'a_to_tsfresh': self.get_a_tsfresh_mapping(mtc_prdcn_table)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60714733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mapping_step(dcRead, s3, spark):\n",
    "    def mapping_load_inputs():\n",
    "        return {\n",
    "            'mtc_prdcn_table': dcRead('prod').loadDataFrame(\n",
    "            'MRP',\n",
    "            'MARKING_THE_CLOSE_PUSH_PRDCN',\n",
    "            'PRC', \n",
    "            'BZ',\n",
    "            '2021-08-05')\n",
    "        }\n",
    "\n",
    "    def mapping_handle_outputs(a_to_tsfresh):\n",
    "        pickle_put(a_to_tsfresh, 'a_to_tsfresh.pickle', s3)\n",
    "\n",
    "    step_mapping = GetMapping(\n",
    "        input_loader=mapping_load_inputs,\n",
    "        output_handler=mapping_handle_outputs\n",
    "    )\n",
    "    return step_mapping\n",
    "\n",
    "mapping_step = make_mapping_step(dcRead, s3, spark)\n",
    "mapping_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('a_to_tsfresh.pickle',s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a58bea",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstructObservations(mrp_pipeline.MRPStep):\n",
    "\n",
    "    name = \"construct_observations\"\n",
    "\n",
    "    def widen_data(self, df):\n",
    "        '''Transform data from initial format to single observation per row format'''\n",
    "\n",
    "        id_1 = df.filter(F.col('id_secondary') == 1) \n",
    "        id_2 = df.filter(F.col('id_secondary') == 2) \n",
    "        id_3 = df.filter(F.col('id_secondary') == 3) \n",
    "        features = id_1.drop(\n",
    "            'id', 'id_secondary', 'LABEL', 'tdate', 'pttrn_trade_dt_all' \n",
    "        ).columns \n",
    "        id_1 = id_1.select(\n",
    "            [F.col(c).alias(f'{c}_1') for c in features] \n",
    "            + ['id', 'id_secondary', 'LABEL', 'tdate', 'pttrn_trade_dt_all']\n",
    "            )\n",
    "        id_2 = id_2.select([F.col(c).alias(f'{c}_2') for c in features] + ['id'])\n",
    "        id_3 = id_3.select([F.col(c).alias(f'{c}_3') for c in features] + ['id']) \n",
    "        \n",
    "        feat_2 = id_2.drop('id_secondary', 'LABEL', 'tdate', 'pttrn_trade_dt_all' ).columns \n",
    "        feat_3 = id_3.drop('id_secondary', 'LABEL', 'tdate', 'pttrn_trade_bot_all').columns \n",
    "        \n",
    "        return id_1.alias('a').join(\n",
    "            id_2.select(feat_2).alias('b').join(\n",
    "                id_3.select(feat_3).alias('c'), \n",
    "                'id', \n",
    "                'inner'\n",
    "            ),\n",
    "            'id', \n",
    "            'inner'\n",
    "        )\n",
    "\n",
    "    def train_val_test_split(self, df, val_percent=0.1, test_percent=0.1):\n",
    "        \"\"\"Split done alphabetically by symbol\"\"\"\n",
    "\n",
    "        df = df.withColumn('symbol', F.split(F.col('id'), '_')[1]) \n",
    "        n_symbols = df.select('symbol').distinct().count() \n",
    "        n_test_symbols = int(test_percent*n_symbols) \n",
    "        n_val_symbols = int(val_percent*n_symbols) \n",
    "        n_train_symbols = n_symbols - n_test_symbols - n_val_symbols\n",
    "        sorted_symbols = sorted(df.select('symbol').distinct().toPandas()['symbol']) \n",
    "        test_symbols = set(sorted_symbols[:n_test_symbols]) \n",
    "        val_symbols = set(\n",
    "            sorted_symbols[n_test_symbols: (n_test_symbols+n_val_symbols)]\n",
    "        )\n",
    "        train_symbols = set (sorted_symbols[(n_test_symbols+n_val_symbols):]) \n",
    "        assert len(train_symbols) == n_train_symbols \n",
    "        assert len(val_symbols) == n_val_symbols \n",
    "        assert len(test_symbols) == n_test_symbols \n",
    "        df = df.withColumn(\n",
    "            'split', \n",
    "            F.when(F.col('symbol').isin(test_symbols), 'test')\n",
    "            .when(F.col('symbol').isin(val_symbols), 'val') \n",
    "            .otherwise ('train')\n",
    "        )\n",
    "        df = df.drop('symbol') \n",
    "        return df\n",
    "\n",
    "    def rename_a_columns_to_close_tsfresh(self, split_data, a_column_mapping):\n",
    "        '''Rename a_? format feature column names to format close to tsfresh names'''\n",
    "\n",
    "        non_feature_cols = ['id', 'id_secondary', 'tdate', 'LABEL', 'pttrn_trade_dt_all', 'split'] \n",
    "        feature_cols = [c for c in split_data.columns if c not in non_feature_cols] \n",
    "        renamed_data = split_data.select(\n",
    "            [F.col(c).alias (a_column_mapping(c)) for c in feature_cols] + non_feature_cols\n",
    "        )\n",
    "        return renamed_data\n",
    "\n",
    "    def main(self, mtc_push_table, a_column_mapping):\n",
    "        wide_data = self.widen_data(mtc_push_table) \n",
    "        split_data = self.train_val_test_split(wide_data) \n",
    "        observations = self.rename_a_columns_to_close_tsfresh(split_data, a_column_mapping) \n",
    "        print('\\n construct_observations > observations:', observations.head(),'\\n',observations.columns.tolist(),'\\n')\n",
    "        return {\n",
    "            'observations': observations\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_co_step(dcRead, s3, spark):\n",
    "    def co_load_inputs():\n",
    "        mtc_push_table = dcRead('prody-dev').loadDataFrame(\n",
    "            \"MRP\",\n",
    "            \"MTC_tsfresh_data_training_data\",\n",
    "            \"PRC\",\n",
    "            \"ORC\",\n",
    "            None\n",
    "        )\n",
    "        a_column_mapping = pickle_get('a_to_tsfresh.pickle', s3)\n",
    "        return {\n",
    "            'mtc_push_table': mtc_push_table,\n",
    "            'a_column_mapping': a_column_mapping\n",
    "        }\n",
    "\n",
    "    def co_handle_outputs(observations):\n",
    "        observations.write.parquet(f'{s3.project_home}/observations.parquet', mode='overwrite')\n",
    "    \n",
    "    step_construct_observations = ConstructObservations(\n",
    "        input_loader=co_load_inputs, \n",
    "        output_handler=co_handle_outputs\n",
    "        )\n",
    "    return step_construct_observations\n",
    "\n",
    "co_step = make_co_step(dcRead, s3, spark)\n",
    "co_step.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8bc5a",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856bbd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceFeatureSet(mrp_pipeline.MRPStep):\n",
    "\n",
    "    name = \"feature_reduction\"\n",
    "    parameters = {\n",
    "        \"variance_threshold\": 0.0001, \n",
    "        \"floating point_error\": 0.000001, \n",
    "        \"vif_threshold\": 10, \n",
    "        \"correlation_threshold\": 0.95, \n",
    "        \"correlation_method\": \"pearson\", \n",
    "        \"soure_tsfresh_datatype_dictionary\" : {'length': 'discrete', 'maximum': 'continuous', 'mean': 'continuous', 'median': 'continuous', 'minimum': 'continuous', 'quantile':'continuous','standard_deviation': 'continuous'}\n",
    "    } \n",
    "\n",
    "    def prep_data(self, split_data):\n",
    "        train_data = split_data.filter (F.col('split') == 'train') \n",
    "        train_data = train_data.drop('split')\n",
    "\n",
    "        X_train , _ = preprocess.preprocess_pre_scale(train_data)\n",
    "\n",
    "        return X_train\n",
    "\n",
    "    def create_tsfresh_feature_reduction_dataframe(self, X_train):\n",
    "        n_features = len(X_train.columns) \n",
    "\n",
    "        data = {'tsfresh_column': X_train.columns,\n",
    "                'datatype': pd.Series('not specified', index = range(n_features)), \n",
    "                'in_current_feature_set': pd.Series(1, index = range(n_features)), \n",
    "                'reason_removed': pd.Series('None', index = range(n_features))\n",
    "                }\n",
    "\n",
    "        tsfresh_feature_reduction_dataframe = pd.DataFrame (data, index = range (n_features))\n",
    "        \n",
    "        # Creates columns datatype\n",
    "        for tsfresh_feature, datatype in self.parameters['soure_tsfresh_datatype_dictionary'].items(): \n",
    "            for index, row in tsfresh_feature_reduction_dataframe.iterrows(): \n",
    "                if tsfresh_feature in row['tsfresh_column']:\n",
    "                    tsfresh_feature_reduction_dataframe['datatype'][index] = datatype\n",
    "        \n",
    "        return tsfresh_feature_reduction_dataframe\n",
    "\n",
    "\n",
    "    def remove_duplicate_features(self, X_train, tsfresh_feature_reduction_dataframe):\n",
    "        relevant_tsfresh_features = tsfresh_feature_reduction_dataframe[tsfresh_feature_reduction_dataframe.in_current_feature_set == 1]['tsfresh_column'].tolist() \n",
    "        X_train_subset = X_train[relevant_tsfresh_features]\n",
    "        \n",
    "        keep_features_vector = X_train_subset.T.drop_duplicates().index\n",
    "\n",
    "        n_features = len(keep_features_vector) \n",
    "        data = {'tsfresh_column': keep_features_vector,\n",
    "                'keep_column': pd.Series(True, index = range(n_features))}\n",
    "\n",
    "        keep_features_dataframe = pd.DataFrame(data, index = range(n_features))\n",
    "        # Update tsfresh_feature_reduction dataframe with results \n",
    "        tsfresh_feature_reduction_dataframe = tsfresh_feature_reduction_dataframe.merge(keep_features_dataframe[['tsfresh_column', 'keep_column']], on = 'tsfresh_column', how = 'left') \n",
    "        tsfresh_feature_reduction_dataframe.loc[(tsfresh_feature_reduction_dataframe. keep_column != True) & (tsfresh_feature_reduction_dataframe.in_current_feature_set == 1),\n",
    "            ['in_current_feature_set', 'reason_removed']] = [0, 'duplicate_feature'] \n",
    "        tsfresh_feature_reduction_dataframe = tsfresh_feature_reduction_dataframe.drop(columns = ['keep_column'])\n",
    "        \n",
    "        return tsfresh_feature_reduction_dataframe\n",
    "\n",
    "\n",
    "    def remove_constant_features (self, X_train, tsfresh_feature_reduction_dataframe) :\n",
    "        relevant_tsfresh_features = tsfresh_feature_reduction_dataframe[tsfresh_feature_reduction_dataframe.in_current_feature_set == 1]['tsfresh_column'].tolist() \n",
    "        X_train_subset = X_train[relevant_tsfresh_features]\n",
    "        \n",
    "        keep_features_vector = VarianceThreshold(threshold = 0).fit(X_train_subset).get_support()\n",
    "        \n",
    "        # Dataframe of results \n",
    "        n_features = len(X_train_subset.columis) \n",
    "        data = {'tsfresh_column': X_train_subset.columns,\n",
    "                'keep_column': keep_features_vector}\n",
    "\n",
    "        keep_features_dataframe = pd. DataFrame (data, index = range(n_features))\n",
    "        # Update tsfresh_feature_reduction_dataframe with results \n",
    "        tsfresh_feature_reduction_dataframe = tsfresh_feature_reduction_dataframe.copy (deep = True).merge (keep_features_dataframe[['tsfresh_column', 'keep_column']],\n",
    "                                                    on = 'tsfresh_column', how = 'left') \n",
    "        tsfresh_feature_reduction_dataframe. loc[(tsfresh_feature_reduction_dataframe. keep_column == False) & (tsfresh_feature_reduction_dataframe.in_current_feature_set == 1),\n",
    "            ['in_current_feature_set', 'reason_removed']] = [0, 'constant feature: variance = 0'] \n",
    "        tsfresh_feature_reduction_dataframe = tsfresh_feature_reduction_dataframe.drop(columns = ['keep_column'])\n",
    "        return tsfresh_feature_reduction_dataframe\n",
    "\n",
    "    def remove_quasi_constant_features(self, X_train, tsfresh_feature_reduction_dataframe):\n",
    "        relevant_tsfresh_features = tsfresh_feature_reduction_dataframe[tsfresh_feature_reduction_dataframe.in_current_feature_set == 1]['tsfresh_column'].tolist() \n",
    "        X_train_subset = X_train[relevant_tsfresh_features]\n",
    "\n",
    "        max_minus_min = X_train_subset.max() - X_train_subset.min()\n",
    "        \n",
    "        n_features = len(X_train_subset.columns) \n",
    "        data = {'tsfresh_column': X_train_subset.columns,\n",
    "            'max minus_min': max_minus_min.values, \n",
    "            'keep_column': pd.Series(True, index = range(n_features))}\n",
    "\n",
    "        keep_features_dataframe = pd. DataFrame (data, index = range(n_features )) \n",
    "        keep_features_dataframe. loc[keep_features_dataframe.max_minus_min <= self.parameters['floating_point_error'], 'keep_column'] = False\n",
    "\n",
    "        # Update tsfresh_feature_reduction_dataframe with results \n",
    "        tsfresh_feature_reduction_dataframe = tsfresh_feature_reduction_dataframe.merge(keep_features_dataframe[['tsfresh_column', 'keep_column']], on = 'tsfresh_column', how = 'left') \n",
    "        tsfresh_feature_reduction_dataframe. loc[(tsfresh_feature_reduction_dataframe.keep_column == False) & (tsfresh_feature_reduction_dataframe.in_current_feature_set == 1),\n",
    "            ['in_current_feature_set', 'reason_removed' ] ] = [0, 'quasi-constant feature'] \n",
    "        tsfresh_feature_reduction_dataframe = tsfresh_feature_reduction_dataframe.drop (columns = ['keep_column'])\n",
    "\n",
    "\n",
    "        return tsfresh_feature_reduction_dataframe\n",
    "\n",
    "    def remove_low_variance_features (self, X_train, tsfresh_feature_reduction_dataframe):\n",
    "        relevant_tsfresh_features = tsfresh_feature_reduction_dataframe[tsfresh_feature_reduction_dataframe.in_current_feature_set == 1]['tsfresh_column'].tolist \n",
    "        X_train_subset = X_train[relevant_tsfresh_features]\n",
    "\n",
    "        scaler = MinMaxScaler().fit(X_train_subset) \n",
    "        scaled_X_train_subset = pd. DataFrame (scaler.transform(X_train_subset), columns = X_train_subset.columns)\n",
    "\n",
    "        # An array of true/false \n",
    "        keep_features_vector = VarianceThreshold(threshold = self.parameters[\"variance_threshold\"]).fit(scaled_X_train_subset).get_support()\n",
    "        \n",
    "        # Dataframe of results \n",
    "        n_features = len(scaled_X_train_subset.columns) \n",
    "        data = {'tsfresh_column': scaled_X_train_subset.columns,\n",
    "                'keep_column': keep_features_vector}\n",
    "\n",
    "\n",
    "        keep_features_dataframe = pd.DataFrame(data, index = range(n_features))\n",
    "        tsfresh_feature_reduction_dataframe = tsfresh_feature_reduction_dataframe.merge(keep_features_dataframe[['tsfresh_column', 'keep_column']], on = 'tsfresh_column', how = 'left') \n",
    "        tsfresh_feature_reduction_dataframe.loc[(tsfresh_feature_reduction_dataframe.keep_column == False) & (tsfresh_feature_reduction_dataframe.in_current_feature_set == 1),\n",
    "            ['in_current_feature_set', 'reason_removed']] = [0, 'low variance feature: min-max scaled variance <= ' + str(self.parameters [\"variance_threshold\"])] \n",
    "        tsfresh_feature_reduction_dataframe = tsfresh_feature_reduction_dataframe.drop(columns = ['keep_column'])\n",
    "        \n",
    "        return tsfresh_feature_reduction_dataframe\n",
    "\n",
    "\n",
    "    def remove_highly_correlated_features (self, X_train, tsfresh_feature_reduction_dataframe): \n",
    "        relevant_tsfresh_features = tsfresh_feature_reduction_dataframe[(tsfresh_feature_reduction_dataframe.in_current_feature_set == 1) &\n",
    "                                                                        (tsfresh_feature_reduction_dataframe.datatype == 'continuous')]['tsfresh_column'].tolist() \n",
    "        X_train_subset = X_train[relevant_tsfresh_features]\n",
    "\n",
    "        # correlation matrix \n",
    "        corr_matrix = X_train_subset.corr(method = self.parameters[\"correlation_method\"]).abs ()\n",
    "        \n",
    "        # Select upper triangle of correlation matrix \n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        \n",
    "        # Find features with correlation greater than threshold \n",
    "        keep_features_vector = [column for column in upper.columns if any (upper[column] >= self.parameters['correlation_thershold'])]\n",
    "        \n",
    "        # Dataframe of results \n",
    "        n_features = len(keep_features_vector) \n",
    "        data = {'tsfresh_column': keep_features_vector,\n",
    "                'keep_column': pd. Series (False, index = range(n_features))} \n",
    "        keep_features_dataframe = pd. DataFrame (data, index = range(n_features))\n",
    "        \n",
    "        tsfresh_feature_reduction_dataframe = tsfresh_feature_reduction_dataframe.merge(keep_features_dataframe[['tsfresh_column', 'keep_column']], on = 'tsfresh_column', how = 'left') \n",
    "        tsfresh_feature_reduction_dataframe. loc[(tsfresh_feature_reduction_dataframe. keep_column == False) & (tsfresh_feature_reduction_dataframe.in_current_feature_set == 1),\n",
    "            ['in_current_feature_set', 'reason_removed']] = [0, 'highly correlated feature: pearson correlation >= ' + str(self.parameters[\"correlation_threshold\"])] \n",
    "        tsfresh_feature_reduction_dataframe = tsfresh_feature_reduction_dataframe.drop(columns = ['keep_column'])\n",
    "        \n",
    "        return tsfresh_feature_reduction_dataframe\n",
    "\n",
    "    def find_highest_vif_feature(self, X, over=None):\n",
    "        '''Find feature with maximum VIF (Variance Inflation Factor).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: pandas.DataFrame\n",
    "            DataFrame whose only columns are continuous features. \n",
    "        over: list[str] (optional)\n",
    "            List of features over which to find the highest-VIF feature. If passed, the VIF \n",
    "            of each feature will still be computed relative to all other features in X, but\n",
    "            the maximum will taken only over this list of features.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        feature_name: str\n",
    "            Name of feature with maximum VIF\n",
    "        vif: float\n",
    "            Maximum VIF\n",
    "        '''\n",
    "\n",
    "        # copy \n",
    "        X = X.copy()\n",
    "        # normalize and drop columns where variance is within rounding error of 0 \n",
    "        X = ((X - X.mean()) / X.std()).dropna(axis=1)\n",
    "\n",
    "        # add a constant column, if there isn't already one there \n",
    "        X = add_constant(X)\n",
    "        \n",
    "        # save column names for later \n",
    "        cols = list(X.columns)\n",
    "\n",
    "        # compute VIFs \n",
    "        X_np = X.to_numpy() \n",
    "        vifs = np.diagonal(np.linalg.pinv(np.matmul(X_np.T, X_np))) * (X.shape[0]-1)\n",
    "\n",
    "        # Combine column names and vifs into dictionary of column names and vifs \n",
    "        vifs_dict = dict(list(zip(cols, vifs)))\n",
    "\n",
    "        # Find highest VIF feature \n",
    "        if over:\n",
    "            # don't consider any feature not in over when taking the maximum vif\n",
    "            vifs_to_consider = [(col, vif) for col, vif in vifs_dict.items() if col in over] \n",
    "        else:\n",
    "            # don't consider the synthetic 'const' feature when taking maximum vif \n",
    "            vifs_to_consider = [(col, vif) for col, vif in vifs_dict.items() if col != 'const']\n",
    "            \n",
    "        sorted_vifs_to_consider = sorted(vifs_to_consider, key=lambda x: x[1])\n",
    "        if len(sorted_vifs_to_consider) > 0:\n",
    "            return sorted_vifs_to_consider(-1) # (feature, vif) with highest vif value \n",
    "        else:\n",
    "            raise ValueError('No features to find VIF for!')\n",
    "        \n",
    "    def remove_high_vif_features (self, X_train, tsfresh_feature_reduction_dataframe) : \n",
    "        relevant_tsfresh_features = tsfresh_feature_reduction_dataframe[(tsfresh_feature_reduction_dataframe.in_current_feature_set == 1) &\n",
    "                                                                        (tsfresh_feature_reduction_dataframe.datatype == 'continuous')]['tsfresh_column'].tolist() \n",
    "        print(f'REMOVING HIGH VIF FEATURES (evaluating {len (relevant_tsfresh_features)} features for removal)')\n",
    "\n",
    "        X_train_subset = X_train[relevant_tsfresh_features]\n",
    "\n",
    "        num_features = len(X_train_subset.columns) \n",
    "        keep_features_dataframe = pd.DataFrame(\n",
    "            {\n",
    "                'tsfresh_column': X_train_subset.columns, 'vif': pd. Series (np.NaN, indest = range(num_features))\n",
    "            },\n",
    "            index = range(num_features)\n",
    "        )\n",
    "\n",
    "        columns_to_maybe_drop = [x for x in relevant_tsfresh_features ] # copy list so original is not modified\n",
    "        \n",
    "        while len(columns_to_maybe_drop) > 0:\n",
    "            then = time.time() \n",
    "            highest_vif_feature, current_max_vif = self.find_highest_vif_feature(X_train_subset, over = columns_to_maybe_drop) \n",
    "            print(f' Current highest VIF is {current_max_vif} for feature {highest_vif_feature}') \n",
    "            keep_features_dataframe. loc[keep_features_dataframe.tsfresh_column == highest_vif_feature, ['vif']] = current_max_vif \n",
    "            if current_max_vif >= self.parameters[\"vif_threshold\"]:\n",
    "                print(f'Dropping feature {highest_vif_feature}') \n",
    "                X_train_subset = X_train_subset.drop(columns = highest_vif_feature)\n",
    "                columns_to_maybe_drop.remove(highest_vif_feature) \n",
    "            else:\n",
    "                print(f'Keeping feature {highest_vif_feature}')\n",
    "                break\n",
    "            columns_to_maybe_drop.remove(highest_vif_feature) \n",
    "            now = time.time() \n",
    "            print(f'Features Evaluated: {len (relevant_tsfresh_features) - len(columns_to_maybe_drop)}/{len (relevant_tsfresh_features)}', end='') \n",
    "            print(' Time elapsed:', now - then)\n",
    "\n",
    "\n",
    "        tsfresh_feature_reduction_dataframe = tsfresh_feature_reduction_dataframe.merge(keep_features_dataframe[['tsfresh_column', 'vif']], on = 'tsfresh_column', how = 'left')\n",
    "        \n",
    "        tsfresh_feature_reduction_dataframe.loc[\n",
    "            (tsfresh_feature_reduction_dataframe.vif >= self.parameters[\"vif_threshold\"]) & (tsfresh_feature_reduction_dataframe.in_current_feature_set == 1),\n",
    "            ['in_current_feature_set', 'reason_removed']\n",
    "        ] = [0, \"high vif: vif >= \" + str(self.parameters [\"vif_threshold\" ])] \n",
    "        tsfresh_feature_reduction_dataframe = tsfresh_feature_reduction_dataframe.drop(columns = ['vif'])\n",
    "        \n",
    "        return tsfresh_feature_reduction_dataframe\n",
    "\n",
    "    def main(self, split_data):\n",
    "        X_train = self.prep_data(split_data)\n",
    "        \n",
    "        tsfresh_feature_reduction_dataframe = self.create_tsfresh_feature_reduction_dataframe(X_train) \n",
    "        tsfresh_feature_reduction_dataframe = self.remove_constant_features(X_train, tsfresh_feature_reduction_dataframe) \n",
    "        tsfresh_feature_reduction_dataframe = self.remove_duplicate_features (X_train, tsfresh_feature_reduction_dataframe) \n",
    "        tsfresh_feature_reduction_dataframe = self.remove_quasi_constant_features(X_train, tsfresh_feature_reduction_dataframe) \n",
    "        #tsfresh_feature_reduction_dataframe = self. remove_low_variance_features (X_train, tsfresh_feature_reduction_dataframe) # don't use \n",
    "        tsfresh_feature_reduction_dataframe = self.remove_highly_correlated_features (X_train, tsfresh_feature_reduction_dataframe)\n",
    "\n",
    "        # vif takes - 4 hrs to run on 1650 features \n",
    "        tsfresh_feature_reduction_dataframe = self.remove_high_vif_features (X_train, tsfresh_feature_reduction_dataframe)\n",
    "        print('\\n feature_reduction > tsfresh_feature_reduction_dataframe:', tsfresh_feature_reduction_dataframe.head(),'\\n',tsfresh_feature_reduction_dataframe.columns.tolist(),'\\n')\n",
    "        return {\n",
    "            'tsfresh_feature_reduction_dataframe': tsfresh_feature_reduction_dataframe\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed0aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_reduction_step(dcRead,\n",
    "    s3,\n",
    "    spark,\n",
    "    variance_threshold = VARIANCE_THRESHOLD,\n",
    "    floating_point_error = FLOATING_POINT_ERROR,\n",
    "    vif_threshold = VIF_THRESHOLD,\n",
    "    correlation_threshold = CORRELATION_THRESHOLD,\n",
    "    correlation_method = CORRELATION_METHOD,\n",
    "    soure_tsfresh_datatype_dictionary = SOURCE_TSFRESH_DATATYPE_DICTIONARY):\n",
    "\n",
    "    def feature_reduction_load_inputs():\n",
    "        return {\n",
    "            'split_data': spark.read.parquet(f'{s3.project_home}/observations.parquet')\n",
    "        }\n",
    "    def feature_reduction_handle_outputs(tsfresh_feature_reduction_dataframe):\n",
    "        pickle_put(tsfresh_feature_reduction_dataframe, 'tsfresh_feature_reduction_dataframe.pickle', s3)\n",
    "\n",
    "    step_feature_reduction = ReduceFeatureSet(\n",
    "        parameters = {\n",
    "            'variance_threshold': variance_threshold,\n",
    "            'floating point_error': floating_point_error,\n",
    "            'vif_threshold': vif_threshold,\n",
    "            'correlation_threshold': correlation_threshold,\n",
    "            'correlation_method': correlation_method,\n",
    "            'soure_tsfresh_datatype_dictionary': soure_tsfresh_datatype_dictionary\n",
    "        },\n",
    "        input_loader = feature_reduction_load_inputs,\n",
    "        output_handler = feature_reduction_handle_outputs\n",
    "    )\n",
    "    return step_feature_reduction \n",
    "\n",
    "feature_reduction_step = make_feature_reduction_step(dcRead, s3, spark)\n",
    "feature_reduction_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25656950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('tsfresh_feature_reduction_dataframe.pickle',s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0706cd7",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedMRMRFit(mrp_pipeline.MRPStep):\n",
    "\n",
    "    name = \"modified_mrmr_fit\"\n",
    "\n",
    "    def prep_data(self, split_data, scale = \"minmax\"):\n",
    "        scaler = preprocess_fit.PreprocessFit().main(split_data, scale)['scaler']\n",
    "\n",
    "        transformed = preprocess_transform.PreprocessTransform().main(split_data, scaler) \n",
    "        X_train = transformed['X_train']\n",
    "        y_train = transformed['y_train']\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "    def create_tsfresh_feature_selection_dataframe(self, tsfresh_feature_reduction_dataframe) :\n",
    "        tsfresh_feature_selection_dataframe = tsfresh_feature_reduction_dataframe[['tsfresh_column', 'datatype', 'in_current_feature_set']]\n",
    "        return tsfresh_feature_selection_dataframe\n",
    "        \n",
    "    def select_mrmr_features(self, X_train, y_train, tsfresh_feature_reduction_dataframe, tsfresh_feature_selection_dataframe):\n",
    "        relevant_tsfresh_features = tsfresh_feature_reduction_dataframe[(tsfresh_feature_reduction_dataframe.in_current_feature_set == 1) & (tsfresh_feature_reduction_dataframe.datatype == 'continuous')]['tsfresh_column'].tolist()\n",
    "        X_train_subset = X_train[relevant_tsfresh_features]\n",
    "\n",
    "        n_features = len(relevant_tsfresh_features) \n",
    "\n",
    "        select_features_vector = mrmr.mrmr_classif(X_train_subset, y_train, K = n_features, return_scores = True, n_jobs = -1)[0]\n",
    "\n",
    "        # Dataframe with results\n",
    "        # tedad featurehaii ke meqdar 'in_current_feature_set' barabar ba 1 darand be onvane n_features estefade mishavand\n",
    "        col = 'select_mrmr_continuous_' + str(n_features) \n",
    "        data = {'tsfresh_column': select_features_vector,\n",
    "                col: pd. Series (range(1, n_features + 1), index = range(n_features))}\n",
    "        \n",
    "        select_features_dataframe = pd.DataFrame (data, index = range(n_features ))\n",
    "        \n",
    "        tsfresh_feature_selection_dataframe = tsfresh_feature_selection_dataframe.copy(deep = True).merge(select_features_dataframe[['tsfresh_column', col]], on = 'tsfresh_column', how = 'left')\n",
    "        \n",
    "        return tsfresh_feature_selection_dataframe\n",
    "\n",
    "    def select_k_best_features(self, X_train, y_train, tsfresh_feature_reduction_dataframe, tsfresh_feature_selection_dataframe, datatype = \"discrete\"):\n",
    "        relevant_tsfresh_features = tsfresh_feature_reduction_dataframe[(tsfresh_feature_reduction_dataframe.in_current_feature_set == 1) & (tsfresh_feature_reduction_dataframe.datatype == datatype)]['tsfresh_column'].tolist() \n",
    "        X_train_subset = X_train[relevant_tsfresh_features]\n",
    "\n",
    "        n_features = len(relevant_tsfresh_features)\n",
    "\n",
    "        if datatype == 'discrete':\n",
    "            score_func = partial(mutual_info_classif, discrete_features = True)\n",
    "\n",
    "        if datatype == \"binary\" :\n",
    "            score_func = chi2\n",
    "\n",
    "        kbest = SelectKBest(score_func = score_func, k = n_features) \n",
    "        kbest.fit(X_train_subset, y_train)\n",
    "\n",
    "        col = 'select_k_best_' + str(datatype) + '_' + str(n_features) \n",
    "        data = {'tsfresh_column': X_train_subset.columns,\n",
    "                'score': kbest.scores_}\n",
    "\n",
    "        select_features_dataframe = pd.DataFrame (data, index = range(n_features)) \n",
    "        select_features_dataframe[col] = select_features_dataframe['score'].rank(ascending = False)\n",
    "\n",
    "        tsfresh_feature_selection_dataframe = tsfresh_feature_selection_dataframe.copy(deep = True).merge( select_features_dataframe[['tsfresh_column', col]], on = 'tsfresh_column', how = 'left')\n",
    "        \n",
    "        return tsfresh_feature_selection_dataframe\n",
    "\n",
    "    def main(self, split_data, tsfresh_feature_reduction_dataframe) :\n",
    "        # y_train on 0-1 scale \n",
    "        X_train_minmax_scaled, y_train = self.prep_data(split_data, scale = \"minmax\")\n",
    "\n",
    "        tsfresh_feature_selection_dataframe = self.create_tsfresh_feature_selection_dataframe(tsfresh_feature_reduction_dataframe)\n",
    "        \n",
    "        # Option 1: \n",
    "        # only numeric \n",
    "        tsfresh_feature_selection_dataframe = self.select_mrmr_features(X_train_minmax_scaled, y_train, tsfresh_feature_reduction_dataframe, tsfresh_feature_selection_dataframe) \n",
    "        # only discrete \n",
    "        tsfresh_feature_selection_dataframe = self.select_k_best_features (X_train_minmax_scaled, y_train, tsfresh_feature_reduction_dataframe, tsfresh_feature_selection_dataframe, datatype = \"discrete\") \n",
    "        # only binary \n",
    "        tsfresh_feature_selection_dataframe = self.select_k_best_features (X_train_minmax_scaled, y_train, tsfresh_feature_reduction_dataframe, tsfresh_feature_selection_dataframe, datatype = \"binary\")\n",
    "        \n",
    "        print('\\n modified_mrmr_fit > tsfresh_feature_selection_dataframe:', tsfresh_feature_selection_dataframe.head(),'\\n',tsfresh_feature_selection_dataframe.columns.tolist(),'\\n')\n",
    "        return {\n",
    "        'tsfresh_feature_selection_dataframe': tsfresh_feature_selection_dataframe\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_modified_mrmr_fit_step(dcRead, s3, spark): \n",
    "    def modified_mrmr_fit_load_inputs():\n",
    "        return {\n",
    "            'split_data': spark.read.parquet (f'{s3.project_home} /observations. parquet'),\n",
    "            'tsfresh_feature_reduction_dataframe': pickle_get('tsfresh_feature_reduction_dataframe.pickle', s3)}\n",
    "\n",
    "    def modified_mrmr_fit_handle_outputs(tsfresh_feature_selection_dataframe):\n",
    "        pickle_put(tsfresh_feature_selection_dataframe, 'tsfresh_feature_selection_dataframe.pickle', s3)\n",
    "        \n",
    "    step_modified_mrmr_fit = ModifiedMRMRFit(\n",
    "        input_loader = modified_mrmr_fit_load_inputs,\n",
    "        output_handler = modified_mrmr_fit_handle_outputs\n",
    "    )\n",
    "    return step_modified_mrmr_fit\n",
    "\n",
    "modified_mrmr_fit_step = make_modified_mrmr_fit_step(dcRead, s3, spark)\n",
    "modified_mrmr_fit_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9719724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('tsfresh_feature_reduction_dataframe.pickle',s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61253f4",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22011b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BorutaFit(mrp_pipeline.MRPStep):\n",
    "    name = \"boruta_fit\"\n",
    "    def prep_data(self, split_data, scale = \"minmax\"):\n",
    "        scaler = preprocess_fit.PreprocessFit().main(split_data, scale)['scaler']\n",
    "        \n",
    "        transformed = preprocess_transform.PreprocessTransform().main(split_data, scaler)\n",
    "        X_train = transformed['X_train']\n",
    "        y_train = transformed['y_train']\n",
    "        return X_train, y_train\n",
    "\n",
    "    def create_tsfresh_feature_selection_dataframe (self, tsfresh_feature_reduction_dataframe) :\n",
    "        tsfresh_feature_selection_dataframe = tsfresh_feature_reduction_dataframe[['tsfresh_column', 'datatype', 'in_current_feature_set']]\n",
    "\n",
    "        return tsfresh_feature_selection_dataframe\n",
    "\n",
    "\n",
    "    def select_boruta_random_forest_features (self, X_train, y_train, tsfresh_feature_reduction_dataframe, tsfresh_feature_selection_dataframe):\n",
    "        relevant_tsfresh_features = tsfresh_feature_reduction_dataframe[(tsfresh_feature_reduction_dataframe.in_current_feature_set == 1)]['tsfresh_column'].tolist() \n",
    "        X_train_subset = X_train[relevant_tsfresh_features]\n",
    "\n",
    "        random_forest = RandomForestClassifier(n_jobs = -1, class_weight = 'balanced', max_depth = 5)\n",
    "        \n",
    "        boruta = BorutaPy(estimator = random_forest, n_estimators = 'auto', max_iter = 100, random_state = 50919) \n",
    "        boruta.fit(np.array(X_train_subset), np.array(y_train))\n",
    "\n",
    "        select_features_vector = X_train_subset.columns[boruta.support_].to_list()\n",
    "        \n",
    "        n_features = len(select_features_vector) \n",
    "        col = 'select_Boruta_' + str(n_features) \n",
    "        data = {'tsfresh_column': select_features_vector,\n",
    "                col: pd. Series (True, indexy= range(n_features))}\n",
    "\n",
    "        select_features_dataframe = pd.DataFrame(data, index = range(n_features))\n",
    "        \n",
    "        tsfresh_feature_selection_dataframe = tsfresh_feature_selection_dataframe.copy(deep = True).merge (select_features_dataframe[['tsfresh_column', col]], on = 'tsfresh_column', how = 'left')\n",
    "        \n",
    "        return tsfresh_feature_selection_dataframe\n",
    "\n",
    "\n",
    "def main(self, split_data, tsfresh_feature_reduction_dataframe) :\n",
    "    # y_train on 0-1 scale \n",
    "    X_train_minmax_scaled, y_train = self.prep_data(split_data, scale = \"minmax\")\n",
    "\n",
    "    tsfresh_feature_selection_dataframe = self.create_tsfresh_feature_selection_dataframe (tsfresh_feature_reduction_dataframe) \n",
    "    tsfresh_feature_selection_dataframe = self.select_Boruta_random_forest_features (X_train_minmax_scaled, y_train, tsfresh_feature_reduction_dataframe, tsfresh_feature_selection_dataframe)\n",
    "    print('\\nboruta_fit outputs: tsfresh_feature_selection_dataframe:', tsfresh_feature_selection_dataframe.head() ,tsfresh_feature_selection_dataframe.columns.tolist(),'\\n')\n",
    "    return {\n",
    "        'tsfresh_feature_selection_dataframe': tsfresh_feature_selection_dataframe\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_boruta_fit_step(dcRead, s3, spark):\n",
    "    def boruta_fit_load_inputs():\n",
    "        return {\n",
    "            'split_data': spark.read.parquet(f'{s3.project_home } /observations. parquet'),\n",
    "            'tsfresh_feature_reduction_dataframe': pickle_get('tsfresh_feature_reduction_dataframe.pickle', s3)}\n",
    "\n",
    "    def boruta_fit_handle_outputs(tsfresh_feature_selection_dataframe) :\n",
    "        pickle_put(tsfresh_feature_selection_dataframe, 'tsfresh_feature_selection_dataframe.pickle', s3)\n",
    "        \n",
    "    step_boruta_fit = BorutaFit(\n",
    "        input_loader = boruta_fit_load_inputs,\n",
    "        output_handler = boruta_fit_handle_outputs)\n",
    "\n",
    "    return step_boruta_fit\n",
    "\n",
    "boruta_fit_step = make_boruta_fit_step(dcRead, s3, spark)\n",
    "boruta_fit_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af11cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('tsfresh_feature_selection_dataframe.pickle',s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c561443d",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectNFeatures(mrp_pipeline.MRPStep):\n",
    "\n",
    "    name = \"select_n_features\"\n",
    "\n",
    "    parameters = {\n",
    "        \"n_features_to_try\": np.round(np.geomspace(10, 500, 12)).astype (int), \n",
    "        \"max_evals\": 50, \n",
    "        \"feature_selection_method\" : 'modified_mrmr'\n",
    "    }\n",
    "\n",
    "    def prep_data(self, n, split_data, tsfresh_feature_selection_dataframe, feature_selection_method): \n",
    "        if feature_selection_method == \"modified_mrmr\":\n",
    "            feature_names = modified_mrmr_tune_n_features.ModifiedMRMRTunenFeatures().main(n, tsfresh_feature_selection_dataframe)['feature_names']\n",
    "        if feature_selection_method == \"boruta\":\n",
    "            feature_names = boruta_tune_n_features.BorutaTuneNFeatures().main(n, tsfresh_feature_selection_dataframe) ['feature_names']\n",
    "        \n",
    "        reduced_data = feature_selection_transform.FeatureSelection_Transform().main(split_data, feature_names)['reduced_data'] \n",
    "        scaler = preprocess_fit.PreprocessFit().main(reduced_data, scale = \"minmax\")('scaler') \n",
    "        data = preprocess_transform. PreprocessTransform().main(reduced_data, scaler)\n",
    "        return data\n",
    "\n",
    "    def get_roc_auc_score(self, model, X, y):\n",
    "        \"\"\"Get model ROC-AUC score on features x and labels y\"\"\"\n",
    "        y_predict = model.predict(X)[:, 1] \n",
    "        roc_auc = sklearn.metrics.roc_auc_score(y, y_predict) \n",
    "        return {'roc_auc': roc_auc,\n",
    "                'y_predict': y_predict\n",
    "        }\n",
    "\n",
    "    def main(self, split_data, tsfresh_feature_selection_dataframe):\n",
    "        train_scores = [] \n",
    "        val_scores = 0 \n",
    "        val_y_predictions = []\n",
    "\n",
    "        for n in self.parameters[\"n_features_to_try\"]:\n",
    "            data = self.prep_data(n, split_data, tsfresh_feature_selection_dataframe, self.parameters[\"feature_selection method\"]) \n",
    "            model = tune_train.TuneTrain(parameters = { 'max_evals': self.parameters['max_evals']}).main(data['X_train'], data['y_train'], data['X_val'], data['y_val'])['model']\n",
    "            \n",
    "            train_roc_auc = self.get_roc_auc_score (model, data[ 'X_train'], data[ 'y_train'])['roc_auc'] \n",
    "            val_roc_auc = self.get_roc_auc_score (model, data['X_val'], data['y_val'])['roc_auc'] \n",
    "            val_y_predict = self.get_roc_auc_score (model, data['X_val'], data['y_val'])['y_predict']\n",
    "            \n",
    "            train_scores.append(train_roc_auc) \n",
    "            val_scores.append(val_roc_auc) \n",
    "            val_y_predictions.append(val_y_predict)\n",
    "\n",
    "            n_feats_best = self.parameters['n_features_to_try'][np.argmax (val_scores)]\n",
    "        print('\\n select_n_features >',{'n_feats_best': n_feats_best, 'n_features_tried': self.parameters['n_features_to_try'], \n",
    "                'train_scores': train_scores,'val_scores': val_scores, 'val_y_predictions': val_y_predictions},'\\n') \n",
    "        return {\n",
    "            'n_feats_best': n_feats_best, \n",
    "            'n_features_tried': self.parameters['n_features_to_try'], \n",
    "            'train_scores': train_scores,\n",
    "            'val_scores': val_scores, \n",
    "            'val_y_predictions': val_y_predictions\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c17007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_select_n_features_step(\n",
    "    dcRead,\n",
    "    s3,\n",
    "    spark,\n",
    "    max_evals = MAX_EVALS,\n",
    "    n_features_to_try = N_FEATURES_TO_TRY,\n",
    "    feature_selection_method = FEATURE_SELECTION_METHOD\n",
    "    ):\n",
    "\n",
    "    def select_n_features_load_inputs():\n",
    "        return {\n",
    "        'split_data': spark.read.parquet(f'{s3.project_home} /observations.parquet'),\n",
    "        'tsfresh_feature_selection_dataframe': pickle_get('tsfresh_feature_selection_dataframe.pickle', s3)}\n",
    "\n",
    "    def select_n_features_handle_outputs (n_feats_best, n_features_tried, train_scores, val_scores, val_y_predictions):\n",
    "        pickle_put(n_feats_best, 'n_feats_best.pickle', s3)\n",
    "        pickle_put(n_features_tried, 'select_n_feats_n_feats.pickle', s3)\n",
    "        pickle_put(train_scores, 'select_n_feats_train scores.pickle', s3)\n",
    "        pickle_put(val_scores, 'select_n_feats_val_scores.pickle', s3)\n",
    "        pickle_put(val_y_predictions, 'val_y_predictions.pickle', s3)\n",
    "\n",
    "    step_select_n_features = SelectNFeatures(\n",
    "        parameters = {\n",
    "            'n_features_to_try': n_features_to_try,\n",
    "            'max_evals': max_evals,\n",
    "            'feature_selection method': feature_selection_method},\n",
    "        input_loader=select_n_features_load_inputs,\n",
    "        output_handler=select_n_features_handle_outputs\n",
    "    )\n",
    "    return step_select_n_features\n",
    "\n",
    "select_n_features_step = make_select_n_features_step(dcRead, s3, spark)\n",
    "select_n_features_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('n_feats_best.pickle',s3)\n",
    "#pickle_get('select_n_feats_n_feats.pickle',s3)\n",
    "#pickle_get('select_n_feats_train.pickle',s3)\n",
    "#pickle_get('select_n_feats_val_scores.pickle',s3)\n",
    "#pickle_get('val_y_predictions.pickle',s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7bea2",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedMRMRTuneNFeatures (mrp_pipeline.MRPStep):\n",
    "    name = \"modified_mrmr_tune_n_features\"\n",
    "    parameters = {\n",
    "        \"fraction_binary\" : 0.09, \n",
    "        \"fraction_discrete\": 0.04,\n",
    "    }\n",
    "    def prep_data(self, tsfresh_feature_selection_dataframe, regex_col):\n",
    "        relevant_tsfresh_dataframe = tsfresh_feature_selection_dataframe.filter(regex = 'select_' + regex_col + 'tsfresh_column').dropna () \n",
    "        feature_names = relevant_tsfresh_dataframe.sort_values(relevant_tsfresh_dataframe.columns[1])['tsfresh_column'].tolist()\n",
    "        return feature_names\n",
    "\n",
    "    def main(self, n, tsfresh_feature_selection_dataframe) :\n",
    "        mrmr_numeric_feature_names = self.prep_data(tsfresh_feature_selection_dataframe, regex_col = 'mrmr') \n",
    "        kbest_binary_feature_names = self.prep_data(tsfresh_feature_selection_dataframe, regex_col = 'k_best_binary') \n",
    "        kbest_discrete_feature_names = self.prep_data(tsfresh_feature_selection_dataframe, regex_col = 'k_best_discrete')\n",
    "        n_binary_features = min(np.ceil(n * self.parameters['fraction_binary']).astype (int), len(kbest_binary_feature_names)) \n",
    "        n_discrete_features = min(np.ceil(n * self.parameters['fraction_discrete']).astype(int), len (kbest_discrete_feature_names)) \n",
    "        n_continuous_features = min(n-n_discrete_features - n_binary_features, len(mrmr_numeric_feature_names))\n",
    "        feature_names = mrmr_numeric_feature_names[0:n_continuous_features] + kbest_discrete_feature_names[0:n_discrete_features] + kbest_binary_feature_names[0:n_binary_features]\n",
    "        print('\\n modified_mrmr_tune_n_features > feature_names:', feature_names,'\\n')\n",
    "        return {\n",
    "            'feature_names': feature_names\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d60bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_modified_mrmr_tune_n_features_step(dcRead, s3, spark):\n",
    "    def modified_mrmr_tune_n_features_load_inputs():\n",
    "        return {\n",
    "            'n': pickle_get('n_feats_best.pickle', s3),\n",
    "            'tsfresh_feature_selection_dataframe': pickle_get('tsfresh_feature_selection_dataframe.pickle', s3)\n",
    "        }\n",
    "    def modified_mrmr_tune_n_features_handle_outputs(feature_names) :\n",
    "        pickle_put(feature_names, 'feature_names.pickle', s3)\n",
    "\n",
    "    step_modified_mrmr_tune_n_features = ModifiedMRMRTuneNFeatures(\n",
    "        input_loader = modified_mrmr_tune_n_features_load_inputs,\n",
    "        output_handler = modified_mrmr_tune_n_features_handle_outputs)\n",
    "\n",
    "    return step_modified_mrmr_tune_n_features\n",
    "\n",
    "modified_mrmr_tune_n_features_step = make_modified_mrmr_tune_n_features_step(dcRead, s3, spark)\n",
    "modified_mrmr_tune_n_features_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ae15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('feature_names.pickle',s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78565844",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93afdcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelectionTransform(mrp_pipeline.MRPStep):\n",
    "    name = \"feature_selection_transform\"\n",
    "    parameters = {\n",
    "        \"non_feature_cols\": ['id', 'id secondary', 'LABEL', 'tdate', 'pttrn_trade_dt_all', 'split']\n",
    "    }\n",
    "\n",
    "    def main(self, split_data, feature_names):\n",
    "        \n",
    "        reduced_data = split_data.select(feature_names + self.parameters['non_feature_cols'])\n",
    "        print('\\n feature_selection_transform > reduced_data:', reduced_data,'\\n')\n",
    "        return {\n",
    "            'reduced_data': reduced_data\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_selection_transform_step(\n",
    "    dcRead,\n",
    "    s3,\n",
    "    spark,\n",
    "    non_feature_cols = NON_FEATURE_COLS):\n",
    "\n",
    "    def feature_selection_transform_load_inputs():\n",
    "        return {\n",
    "        'split_data': spark.read.parquet(f'{s3.project_home}/observations.parquet'),\n",
    "        'feature_names': pickle_get('feature_names.pickle', s3)}\n",
    "\n",
    "    def feature_selection_transform_handle_outputs(reduced_data): \n",
    "        return { \n",
    "            'reduced_data': reduced_data.write.parquet(\n",
    "            f'{s3.project_home} /reduced_data.parquet', \n",
    "            mode='overwrite')\n",
    "        }\n",
    "\n",
    "    step_mrmr_transform = FeatureSelectionTransform(\n",
    "        parameters = {\n",
    "            'non_feature_cols': non_feature_cols},\n",
    "        input_loader=feature_selection_transform_load_inputs,\n",
    "        output_handler=feature_selection_transform_handle_outputs\n",
    "    )\n",
    "    return step_mrmr_transform\n",
    "\n",
    "feature_selection_transform_step = make_feature_selection_transform_step(dcRead, s3, spark)\n",
    "feature_selection_transform_step.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33585c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessFit(mrp_pipeline.MRPStep):\n",
    "\n",
    "    name = \"preprocess_fit\"\n",
    "    \n",
    "    def main(self, split_data, scale = \"minmax\"):\n",
    "        train_data = split_data.filter (F.col('split') == 'train') \n",
    "        train_data = train_data.drop('split')\n",
    "        X_train, _ = preprocess.preprocess_pre_scale(train_data)\n",
    "        if scale == \"minmax\":\n",
    "            scaler = MinMaxScaler().fit(X_train) \n",
    "        if scale == \"standard\":\n",
    "            scaler = StandardScaler().fit(X_train)\n",
    "        print('\\n preprocess_fit > scaler:', scaler,'\\n') \n",
    "        return {\n",
    "            'scaler': scaler\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocess_fit_step(dcRead, s3, spark):\n",
    "    def pf_load_inputs():\n",
    "        return {\n",
    "        'split_data': spark.read.parquet (f'{s3.project_home} /reduced_data.parquet')\n",
    "        }\n",
    "    def pf_handle_outputs(scaler):\n",
    "        pickle_put (scaler, 'min_max_scaler.pickle', s3)\n",
    "\n",
    "    step_preprocess_fit = PreprocessFit(\n",
    "        input_loader = pf_load_inputs,\n",
    "        output_handler = pf_handle_outputs)\n",
    "\n",
    "    return step_preprocess_fit\n",
    "\n",
    "preprocess_fit_step = make_preprocess_fit_step(dcRead, s3, spark)\n",
    "preprocess_fit_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d082afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('min_max_scaler.pickle',s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65b4bd4",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f471ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess_Transform(mrp_pipeline.MRPStep):\n",
    "\n",
    "    name = \"preprocess_transform\"\n",
    "\n",
    "    def apply(self, data, scaler):\n",
    "        X, y = preprocess.preprocess_pre_scale(data) \n",
    "        X_scaled = pd.DataFrame (scaler.transform(X), columns=X.columns) \n",
    "        return X_scaled, y\n",
    "\n",
    "    def main(self, split_data, scaler):\n",
    "        train_data = split_data.filter (F.col('split') == 'train') \n",
    "        val_data = split_data.filter (F.col('split') == 'val') \n",
    "        test_data = split_data.filter (F.col('split') == 'test') \n",
    "        train_data = train_data.drop('split') \n",
    "        val_data = val_data.drop('split') \n",
    "        test_data = test_data.drop('split') \n",
    "        X_train, y_train = self.apply(train_data, scaler) \n",
    "        X_val, y_val = self.apply(val_data, scaler) \n",
    "        X_test, y_test = self.apply(test_data, scaler)\n",
    "        print('\\n preprocess_transform >',{'X val': X_val,'y_val': y_val, \n",
    "                                            'X_test': X_test, 'y_test': y_test},'\\n') \n",
    "        return {\n",
    "            'X_train': X_train, \n",
    "            'y_train': y_train,\n",
    "            'X val': X_val, \n",
    "            'y_val': y_val, \n",
    "            'X_test': X_test, \n",
    "            'y_test': y_test\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocess_transform_step(dcRead, s3, spark):\n",
    "    def pt_load_inputs():\n",
    "        return {\n",
    "            'split_data': spark.read.parquet (f'{s3.project_home} /reduced_data.parquet'),\n",
    "            'scaler': pickle_get('min_max_scaler. pickle', s3)}\n",
    "\n",
    "    def pt_handle_outputs(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        X_test,\n",
    "        y_test):\n",
    "\n",
    "        pickle_put(X_train, 'X_train.pickle', s3)\n",
    "        pickle_put(y_train, 'y_train.pickle', s3)\n",
    "        pickle_put(X_val, 'x_val.pickle', s3) \n",
    "        pickle_put(y_val, 'y_val.pickle', s3)\n",
    "        pickle_put(X_test, 'X_test.pickle', s3)\n",
    "        pickle_put(y_test, 'y_test.pickle', s3)\n",
    "\n",
    "    step_preprocess_transform = PreprocessTransform(\n",
    "        input_loader=pt_load_inputs, \n",
    "        output_handler=pt_handle_outputs)\n",
    "\n",
    "    return step_preprocess_transform\n",
    "\n",
    "preprocess_transform_step = make_preprocess_transform_step(dcRead, s3, spark)\n",
    "preprocess_transform_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72260dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('X_train.pickle',s3)\n",
    "#pickle_get('y_train.pickle',s3)\n",
    "#pickle_get('x_val.pickle',s3)\n",
    "#pickle_get('y_val.pickle',s3)\n",
    "#pickle_get('X_test.pickle',s3)\n",
    "#pickle_get('y_test.pickle',s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d6f06b",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneTrain(mrp_pipeline.MRPStep):\n",
    "    parameters = {\n",
    "    'max_evals': 50\n",
    "    }\n",
    "    name = \"tune_train\"\n",
    "    def define_search_space (self):\n",
    "        \"\"\"Define a hyperparameter search space for hyperopt to optimize over\"\"\"\n",
    "        search_space = {} \n",
    "        search_space['intro_neurons'] = hp.quniform( \"intro_neurons\", 500, 1000, 1) \n",
    "        search_space['activations_intro'] = hp.choice(\n",
    "            \"activations_intro\", \n",
    "            ['relu','elu', 'selu']\n",
    "        )\n",
    "        search_space['number_layers'] = hp.choice(\n",
    "            'number_layers',\n",
    "            [\n",
    "                {\n",
    "                    'number_layers': 0, \n",
    "                    'num_neurons_1': hp.quniform(\n",
    "                        \"num_neurons_1\", \n",
    "                        1, \n",
    "                        1000, \n",
    "                        1\n",
    "                    ),\n",
    "                    \"activations_1\": hp.choice(\n",
    "                        \"activations_1\", ['relu', 'elu', 'selu']\n",
    "                    ),\n",
    "                    'learning_rate': hp.loguniform(\n",
    "                        'learning_rate_zero', \n",
    "                        np.log(0.1), \n",
    "                        np.log(1.4)\n",
    "                    )\n",
    "                },\n",
    "                {      \n",
    "                    'number_layers': 1, \n",
    "                    'num_neurons_second_1': hp.quniform(\n",
    "                        'num_neurons_second_1',\n",
    "                        1,\n",
    "                        1000,\n",
    "                        1\n",
    "                    ),\n",
    "                    'num_neurons_second_2' :hp.quniform(\n",
    "                        'num_neurons_second_2', \n",
    "                        1, \n",
    "                        1000,\n",
    "                        1\n",
    "                    ),\n",
    "                    \"activations_2_first\": hp.choice(\n",
    "                        \"activations_2_first\", \n",
    "                        ['relu','elu', 'selu']\n",
    "                    ),\n",
    "                    \"activations_2_second\": hp.choice(\n",
    "                        \"activations_2_second\", \n",
    "                        ['relu', 'elu', 'selu']\n",
    "                    ),\n",
    "                    'learning_rate': hp.loguniform(\n",
    "                        'learning_rate_one', \n",
    "                        np.log(0.1), \n",
    "                        np.log(1.4)\n",
    "                    )\n",
    "                },\n",
    "                {\n",
    "                    'number_layers' : 2, \n",
    "                    'num_neurons_3_1': hp.quniform(\n",
    "                        \"num_neurons_3_1\",\n",
    "                        1,\n",
    "                        1000,\n",
    "                        1\n",
    "                    ),\n",
    "                    'num_neurons_3_2': hp.quniform(\n",
    "                        \"num_neurons_3_2\",\n",
    "                        1,\n",
    "                        800,\n",
    "                        1\n",
    "                    ),\n",
    "                    'num_neurons_3_3': hp.quniform(\n",
    "                        \"num_neurons_3_3\",\n",
    "                        1,\n",
    "                        1000, \n",
    "                        10\n",
    "                    ),\n",
    "                    \"activations_3_1\" : hp.choice(\n",
    "                        \"activations_3_1\", \n",
    "                        ['relu', 'elu', 'selu']\n",
    "                    ),\n",
    "                    \"activations_3_2\": hp.choice(\n",
    "                        \"activations_3_2\", \n",
    "                        ['relu', 'elu', 'selu']\n",
    "                    ),\n",
    "                    \"activations_3_3\": hp.choice(\n",
    "                        \"activations_3_3\", \n",
    "                        ['relu', 'elu', 'selu']\n",
    "                    ),\n",
    "                    'learning_rate': hp. loguniform(\n",
    "                        'learning_rate_two', \n",
    "                        np.log(0.1), \n",
    "                        np.log(1.4)\n",
    "                    )\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "        return search_space\n",
    "\n",
    "    def objective(self, X_val, X_train, y_binary_val, y_binary_train, params):\n",
    "        \"\"\"Train a model and get its performance to use for hyperparameter tuning\"\"\"\n",
    "        classifier = tf.keras.Sequential() \n",
    "        es_p = tf.keras.callbacks. EarlyStopping(\n",
    "            monitor='val_auc', \n",
    "            verbose=1, mode='max', \n",
    "            patience=50, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        es_1 = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            verbose=1, \n",
    "            mode='min', \n",
    "            patience=50,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # set up the input layer and actually the first hidden layer \n",
    "        classifier.add(tf.keras.layers.Dropout(\n",
    "            0.2, \n",
    "            input_shape=(X_train.shape[-1],)\n",
    "        ))\n",
    "        classifier.add(tf.keras.layers.Dense(\n",
    "                params['intro_neurons'], \n",
    "                activation=params['activations_intro']\n",
    "        ))\n",
    "        if params['number_layers'] == 0:\n",
    "            classifier.add(tf.keras.layers.Dropout(0.6)) \n",
    "            classifier.add(tf.keras.layers.Dense(\n",
    "                params['number_layers']['num_neurons_1'], \n",
    "                activation=params['number_layers']['activations_1'], \n",
    "                kernel_constraint=MaxNorm(3)\n",
    "            ))\n",
    "\n",
    "        if params['number_layers'] == 1:\n",
    "            classifier.add(tf.keras.layers.Dropout(0.6)) \n",
    "            classifier.add(tf.keras.layers.Dense(\n",
    "                params['number_layers']['num_neurons_second_1'], \n",
    "                activation=params['number_layers']['activations_2_first'], \n",
    "                kernel_constraint = MaxNorm(3)\n",
    "            ))\n",
    "            classifier.add(tf.keras.layers.Dropout(0.6)) \n",
    "            classifier.add(tf.keras.layers.Dense(\n",
    "                params['number_layers']['num neurons second 2'], \n",
    "                activation=params['number_layers']['activations_2_second' ], \n",
    "                kernel_constraint=MaxNorm(3)\n",
    "            ))\n",
    "\n",
    "        if params['number_layers'] == 2:\n",
    "            classifier.add(tf.keras.layers.Dropout(0.6)) \n",
    "            classifier.add(tf.keras.layers.Dense(\n",
    "                params['number_layers']['num_neurons_3_1'], \n",
    "                activation=params['number_layers']['activations_3_1'], \n",
    "                kernel_constraint=MaxNorm(3)\n",
    "            ))\n",
    "            classifier.add(tf.keras.layers.Dropout(0.6)) \n",
    "            classifier.add(tf.keras.layers.Dense(\n",
    "                params['number_layers']['num_neurons_3_2'], \n",
    "                activation=params['number_layers']['activations_3_2'],\n",
    "                    kernel_constraint=MaxNorm(3) \n",
    "                )) \n",
    "            classifier.add(tf.keras.layers.Dropout(0.6)) \n",
    "            classifier.add(tf.keras.layers.Dense(\n",
    "                params['number_layers']['num_neurons_3_3'], \n",
    "                activation=params['number_layers']['activations_3_3'], \n",
    "                kernel_constraint = MaxNorm(3)\n",
    "            ))\n",
    "        classifier.add(tf.keras.layers.Dense(\n",
    "            2,\n",
    "            activation='softmax', \n",
    "            kernel_constraint = MaxNorm(3)\n",
    "        ))\n",
    "        optimizer = SGD(\n",
    "            lr = params['number_layers']['learning_rate'], \n",
    "            momentum=0.9\n",
    "        )\n",
    "        classifier.compile(\n",
    "            loss = tf.keras.losses.categoricalCrossentropy(), \n",
    "            optimizer=optimizer, \n",
    "            metrics=[tf.keras.metrics.AUC(name='auc')]\n",
    "        )            \n",
    "        classifier.fit(\n",
    "            X_train, \n",
    "            y_binary_train, \n",
    "            epochs=50, \n",
    "            validation_data = (X_val, y_binary_val), \n",
    "            verbose=2, \n",
    "            batch_size = 64, \n",
    "            callbacks=[es_p, es_1]\n",
    "        )\n",
    "        score_2 = classifier.evaluate (X_val, y_binary_val) \n",
    "        return {'loss': - score_2[1], 'status': STATUS_OK, 'model': classifier}\n",
    "\n",
    "    def obj_with_data(self, X_val, X_train, y_binary_val, y_binary_train): \n",
    "        return lambda params: self.objective(\n",
    "            X_val, X_train, y_binary_val, y_binary_train, params\n",
    "        )\n",
    "\n",
    "    def getBestModelfromTrials(self, trials): \n",
    "        valid_trial_list = [\n",
    "            trial for trial in trials if STATUS_OK == trial['result']['status']\n",
    "        ]\n",
    "        losses = [float(trial['result']['loss']) for trial in valid_trial_list] \n",
    "        index_having_minumum_loss = np.argmin (losses) \n",
    "        best_trial_obj = valid_trial_list[index_having_minumum_loss ] \n",
    "        return best_trial_obj['result']['model']\n",
    "        \n",
    "    def main(self, X_train, y_train, X_val, y_val):\n",
    "        y_binary_train = to_categorical(y_train.values) \n",
    "        y_binary_val = to_categorical(y_val.values) \n",
    "        search_space = self.define_search_space() \n",
    "        algo = tpe.suggest \n",
    "        trials = Trials() \n",
    "        best = fmin(\n",
    "            self.obj_with_data(X_val, X_train, y_binary_val, y_binary_train),\n",
    "            search_space, \n",
    "            algo=algo, \n",
    "            trials=trials, \n",
    "            max_evals=self.parameters['max_evals']\n",
    "        )\n",
    "        model = self.getBestModelfromTrials (trials) \n",
    "        print('\\n tune_train > model:', model,'\\n') \n",
    "        return {\n",
    "            'model': model\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1956f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tune_train_step(dcRead, s3, spark, max_evals=MAX_EVALS_TUNE_TRAIN):\n",
    "\n",
    "    def tt_load_inputs():\n",
    "        return{\n",
    "        'X_train': pickle_get('X_train.pickle', s3),\n",
    "        'y_train': pickle_get('y_train.pickle', s3),\n",
    "        'X_val': pickle_get('X_val.pickle', s3),\n",
    "        'y_val': pickle_get('y_val.pickle', s3)}\n",
    "\n",
    "    def tt_handle_outputs (model):\n",
    "        keras_put(model, 'model.h5', s3) \n",
    "\n",
    "    step_tune_train = TuneTrain(\n",
    "        parameters={ \n",
    "            'max_evals': max_evals \n",
    "        },\n",
    "        input_loader=tt_load_inputs, \n",
    "        output_handler=tt_handle_outputs\n",
    "    )\n",
    "    return step_tune_train\n",
    "\n",
    "tune_train_step = make_tune_train_step(dcRead, s3, spark)\n",
    "tune_train_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras_put(model, 'model.h5', s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fea1f9",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a7adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate(mrp_pipeline.MRPStep):\n",
    "\n",
    "    name = \"evaluate\"\n",
    "\n",
    "    def main(self, model, X_test, y_test):\n",
    "        y_predict = model.predict(X_test)[:, 1] \n",
    "        y_binary_predict = (y_predict > 0.5).astype (float) \n",
    "        metrics_dict = {\n",
    "            'recall': [sklearn.metrics.recall_score(y_test, y_binary_predict)],\n",
    "            'precision': [sklearn.metrics.precision_score(y_test, y_binary_predict)], \n",
    "            'roc_auc': [sklearn.metrics.roc_auc_score(y_test, y_predict)]\n",
    "        }\n",
    "        print('\\n evaluate > ', metrics_dict,'\\n')\n",
    "        return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ffd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_evaluate_step (dcRead, s3, spark):\n",
    "    def eval_load_inputs():\n",
    "        return {\n",
    "            'model': keras_get('model.h5', s3),\n",
    "            'X_test': pickle_get('X_test.pickle', s3),\n",
    "            'y_test': pickle_get('y_test.pickle', s3)}\n",
    "\n",
    "    def eval_handle_outputs (recall, precision, roc_auc):\n",
    "        print(f'Recall: {recall}')\n",
    "        print(f'Precision: {precision}')\n",
    "        print(f'ROC-AUC: {roc_auc}')\n",
    "    \n",
    "    step_evaluate = Evaluate(\n",
    "        input_loader = eval_load_inputs,\n",
    "        output_handler = eval_handle_outputs\n",
    "    )\n",
    "    return step_evaluate\n",
    "\n",
    "evaluate_step = make_evaluate_step(dcRead, s3, spark)\n",
    "evaluate_step.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca84db4",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831162d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompileMetadata (mrp_pipeline.MRPStep):\n",
    "\n",
    "    name = \"compile_metadata\"\n",
    "    \n",
    "    def main(self, simplified_tsfresh_map, selected_features, scaler):\n",
    "        metadata = {} \n",
    "        metadata.update({\n",
    "            'simplified_tsfresh_to_tsfresh' : {\n",
    "                'description': 'simplifed tsfresh name to tsfresh name mapping', \n",
    "                'data': simplified_tsfresh_map\n",
    "            }\n",
    "        })\n",
    "        metadata.update({ \n",
    "            'selected_features' : {\n",
    "                'description': 'Selected features in simplified tsfresh name format', \n",
    "                'data': selected_features\n",
    "            }\n",
    "        })\n",
    "        scaling = [\n",
    "            {\n",
    "                'feature': selected_features[i], \n",
    "                'min': scaler.data_min_[i], \n",
    "                'max': scaler.data_max_[i]\n",
    "            }\n",
    "            for i in range (len(selected_features))\n",
    "        ]\n",
    "        metadata.update({ \n",
    "        'scaling': {\n",
    "                'description': 'Min and Max values for each selected feature with features in simplified tsfresh name format', \n",
    "                'data': scaling\n",
    "            }\n",
    "        })\n",
    "\n",
    "        print('\\n compile_metadata > metadata:', metadata,'\\n')\n",
    "        return {\n",
    "            'metadata': metadata\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79743e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_compile_metadata_step(dcRead, s3, spark):\n",
    "    def cm_load_inputs():\n",
    "        return {\n",
    "            'simplified_tsfresh_map': pickle_get('full_simplified_tsfresh_map.pickle', s3),\n",
    "            'selected_features': pickle_get('feature_names.pickle', s3),\n",
    "            'scaler': pickle_get('min_max_scaler.pickle', s3)\n",
    "        }\n",
    "\n",
    "    def cm_handle_outputs (metadata):\n",
    "        json_put(metadata, 'metadata.json', s3)\n",
    "        \n",
    "    step_cm = CompileMetadata(\n",
    "        input_loader=cm_load_inputs,\n",
    "        output_handler=cm_handle_outputs)\n",
    "    \n",
    "    return step_cm\n",
    "\n",
    "compile_metadata_step = make_compile_metadata_step(dcRead, s3, spark)\n",
    "compile_metadata_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff69af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_get('metadata.json', s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66b052",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage (mrp_pipeline.MRPStep):\n",
    "\n",
    "    name = \"stage\"\n",
    "\n",
    "    def main(self, split_data, metadata, model, run_info): \n",
    "        print('\\n stage >',{'split_data': split_data,'run_info': run_info},'\\n') \n",
    "        return {\n",
    "            'split_data': split_data, \n",
    "            'metadata': metadata, \n",
    "            'model': model, \n",
    "            'run_info': run_info\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3827ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stage_step(dcRead, s3, spark):\n",
    "    def get_run_info() :\n",
    "        with open('/mnt/metadata/run_information.json', 'r') as f:\n",
    "            run_info = json.load(f)\n",
    "        return run_info\n",
    "\n",
    "    def stage_load_inputs():\n",
    "        return {\n",
    "            'split_data': spark.read.parquet (f'{s3.project_home} /observations.parquet'),\n",
    "            'metadata': json_get('metadata.json', s3),\n",
    "            'model': keras_get('model.h5', s3),\n",
    "            'run_info': get_run_info()\n",
    "        }\n",
    "\n",
    "    def stage_handle_outputs (split_data, metadata, model, run_info):\n",
    "        split_data.write.parquet(\n",
    "            f'{s3.project_home}/staged/split_data. parquet',\n",
    "            mode='overwrite'\n",
    "        )\n",
    "        json_put (metadata, 'staged/metadata.json', s3) \n",
    "        keras_put (model, 'staged/model.h5', s3)\n",
    "        json_put (run_info, 'staged/run_info.json', s3)\n",
    "\n",
    "    step_stage = Stage(\n",
    "        input_loader = stage_load_inputs,\n",
    "        output_handler = stage_handle_outputs\n",
    "    )    \n",
    "    return step_stage\n",
    "\n",
    "\n",
    "stage_step = make_stage_step(dcRead, s3, spark)\n",
    "stage_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb5491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_get('staged/metadata.json', s3)\n",
    "#keras_put(model, 'model.h5', s3)\n",
    "#json_get('staged/run_info.json', s3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
