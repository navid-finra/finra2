{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fee632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import mrmr\n",
    "import time\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, IntegerType, DoubleType \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2,VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "from boruta import BorutaPy\n",
    "from random import sample\n",
    "from functools import partial\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce57f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/deep learning/utilities')\n",
    "\n",
    "from spark_setup import SetupEnvironment\n",
    "environment = SetupEnvironment (conda_env='my_env')\n",
    "spark = environment.setup_spark()\n",
    "dcRead = environment.setup_DataCatalog()\n",
    "s3 = environment.setup_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/repos/mme010/DS_Artifacts/prototypes/V2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db27f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from push_training.common import mrp_pipeline , preprocess\n",
    "\n",
    "from push_training.steps import (\n",
    "    construct_observations, feature_reduction, modified_mrmr_fit,\n",
    "    boruta_fit, select_n_features, modified_mrmr_tune_n_features,\n",
    "    feature_selection_transform, preprocess_fit, preprocess_transform,\n",
    "    tune_train, evaluate, get_a_tsfresh_mapping, get_tsfresh_simplified_mapping,\n",
    "    compile_metadata, stage \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "VARIANCE_THRESHOLD = 0.0001\n",
    "FLOATING_POINT_ERROR = 0.000001\n",
    "VIF_THRESHOLD = 10\n",
    "CORRELATION_THRESHOLD = 0.95 \n",
    "CORRELATION_METHOD = \"pearson\"\n",
    "MAX_EVALS_TUNE_TRAIN = 300\n",
    "N_FEATURES_TO_TRY = np.round(np.geomspace (10, 500, 16)).astype(int)\n",
    "MAX_EVALS = 50\n",
    "FEATURE_SELECTION_METHOD = \"modified_mrmr\" #boruta\n",
    "NON_FEATURE_COLS = ['id', 'id_secondary', 'LABEL', 'tdate', 'pttrn_trade_dt_all' , 'split']\n",
    "SOURCE_TSFRESH_DATATYPE_DICTIONARY = {'length': 'discrete', 'maximum': 'continuous', 'mean':'continuous', 'median':'continuous','minimum':'continuous','quantile':'continuous'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb7199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_put(data, name, s3):\n",
    "    with s3.open(f'{s3.project_home}/{name}', 'wb') as f: \n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def pickle_get(name, s3):\n",
    "    with s3.open(f'{s3.project_home}/{name}', 'rb') as f: \n",
    "        return pickle.load(f)\n",
    "\n",
    "def keras_put(model, name, s3): \n",
    "    tmp_path='/tmp/tmp_model.h5'\n",
    "    model.save(tmp_path)\n",
    "    s3.put(tmp_path, f'{s3.project_home}/{name}')\n",
    "\n",
    "def keras_get(name, s3): \n",
    "    tmp_path ='/tmp/tmp_model/h5'\n",
    "    s3.get(f'{s3.project_home}/{name}', tmp_path) \n",
    "    model = tf.keras.models.load_model(tmp_path)\n",
    "    return model\n",
    "\n",
    "def json_put(data, name, s3):\n",
    "    with s3.open(f'{s3.project_home}/{name}', 'w') as f: \n",
    "        json.dump(data, f)\n",
    "\n",
    "def json_get(name, s3):\n",
    "    with s3.open(f'{s3.project_home}/{name}', 'r') as f: \n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2025db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tsfresh_simplified_mapping_step(dcRead, s3, spark):\n",
    "    def ts_mapping_load_inputs():\n",
    "        return {\n",
    "            'mi_feature_metadata': json.load(open('/repos/mme010/DS_Artifacts/prototypes/v2/push_training/momentum_ignition_feature_metadata.json', 'r'))\n",
    "        }\n",
    "        \n",
    "    def ts_mapping_handle_outputs(full_simplified_tsfresh_map): \n",
    "        pickle_put(\n",
    "            full_simplified_tsfresh_map, \n",
    "            'full_simplified_tsfresh_map.pickle', \n",
    "            s3)\n",
    "\n",
    "    step_ts_mapping = get_tsfresh_simplified_mapping.GetTsfreshSimplifiedMapping(\n",
    "        input_loader=ts_mapping_load_inputs, \n",
    "        output_handler=ts_mapping_handle_outputs)\n",
    "    \n",
    "    return step_ts_mapping\n",
    "\n",
    "tsfresh_simplified_mapping_step = make_tsfresh_simplified_mapping_step(dcRead, s3, spark)\n",
    "tsfresh_simplified_mapping_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9261b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('full_simplified_tsfresh_map.pickle',s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca02b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mapping_step(dcRead, s3, spark):\n",
    "    def mapping_load_inputs():\n",
    "        return {\n",
    "            'mtc_prdcn_table': dcRead('prod').loadDataFrame(\n",
    "            'MRP',\n",
    "            'MARKING_THE_CLOSE_PUSH_PRDCN',\n",
    "            'PRC', \n",
    "            'BZ',\n",
    "            '2021-08-05')\n",
    "        }\n",
    "\n",
    "    def mapping_handle_outputs(a_to_tsfresh):\n",
    "        pickle_put(a_to_tsfresh, 'a_to_tsfresh.pickle', s3)\n",
    "\n",
    "    step_mapping = get_a_tsfresh_mapping.GetMapping(\n",
    "        input_loader=mapping_load_inputs,\n",
    "        output_handler=mapping_handle_outputs\n",
    "    )\n",
    "    return step_mapping\n",
    "\n",
    "mapping_step = make_mapping_step(dcRead, s3, spark)\n",
    "mapping_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82694785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('a_to_tsfresh.pickle',s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_co_step(dcRead, s3, spark):\n",
    "    def co_load_inputs():\n",
    "        mtc_push_table = dcRead('prody-dev').loadDataFrame(\n",
    "            \"MRP\",\n",
    "            \"MTC_tsfresh_data_training_data\",\n",
    "            \"PRC\",\n",
    "            \"ORC\",\n",
    "            None\n",
    "        )\n",
    "        a_column_mapping = pickle_get('a_to_tsfresh.pickle', s3)\n",
    "        return {\n",
    "            'mtc_push_table': mtc_push_table,\n",
    "            'a_column_mapping': a_column_mapping\n",
    "        }\n",
    "\n",
    "    def co_handle_outputs(observations):\n",
    "        observations.write.parquet(f'{s3.project_home}/observations.parquet', mode='overwrite')\n",
    "    \n",
    "    step_construct_observations = construct_observations.ConstructObservations(\n",
    "        input_loader=co_load_inputs, \n",
    "        output_handler=co_handle_outputs\n",
    "        )\n",
    "    return step_construct_observations\n",
    "\n",
    "co_step = make_co_step(dcRead, s3, spark)\n",
    "co_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_reduction_step(dcRead,\n",
    "    s3,\n",
    "    spark,\n",
    "    variance_threshold = VARIANCE_THRESHOLD,\n",
    "    floating_point_error = FLOATING_POINT_ERROR,\n",
    "    vif_threshold = VIF_THRESHOLD,\n",
    "    correlation_threshold = CORRELATION_THRESHOLD,\n",
    "    correlation_method = CORRELATION_METHOD,\n",
    "    soure_tsfresh_datatype_dictionary = SOURCE_TSFRESH_DATATYPE_DICTIONARY):\n",
    "\n",
    "    def feature_reduction_load_inputs():\n",
    "        return {\n",
    "            'split_data': spark.read.parquet(f'{s3.project_home}/observations.parquet')\n",
    "        }\n",
    "    def feature_reduction_handle_outputs(tsfresh_feature_reduction_dataframe):\n",
    "        pickle_put(tsfresh_feature_reduction_dataframe, 'tsfresh_feature_reduction_dataframe.pickle', s3)\n",
    "\n",
    "    step_feature_reduction = feature_reduction.ReduceFeatureSet(\n",
    "        parameters = {\n",
    "            'variance_threshold': variance_threshold,\n",
    "            'floating point_error': floating_point_error,\n",
    "            'vif_threshold': vif_threshold,\n",
    "            'correlation_threshold': correlation_threshold,\n",
    "            'correlation_method': correlation_method,\n",
    "            'soure_tsfresh_datatype_dictionary': soure_tsfresh_datatype_dictionary\n",
    "        },\n",
    "        input_loader = feature_reduction_load_inputs,\n",
    "        output_handler = feature_reduction_handle_outputs\n",
    "    )\n",
    "    return step_feature_reduction \n",
    "\n",
    "feature_reduction_step = make_feature_reduction_step(dcRead, s3, spark)\n",
    "feature_reduction_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('tsfresh_feature_reduction_dataframe.pickle',s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_modified_mrmr_fit_step(dcRead, s3, spark): \n",
    "    def modified_mrmr_fit_load_inputs():\n",
    "        return {\n",
    "            'split_data': spark.read.parquet (f'{s3.project_home} /observations. parquet'),\n",
    "            'tsfresh_feature_reduction_dataframe': pickle_get('tsfresh_feature_reduction_dataframe.pickle', s3)}\n",
    "\n",
    "    def modified_mrmr_fit_handle_outputs(tsfresh_feature_selection_dataframe):\n",
    "        pickle_put(tsfresh_feature_selection_dataframe, 'tsfresh_feature_selection_dataframe.pickle', s3)\n",
    "        \n",
    "    step_modified_mrmr_fit = modified_mrmr_fit.ModifiedMRMRFit(\n",
    "        input_loader = modified_mrmr_fit_load_inputs,\n",
    "        output_handler = modified_mrmr_fit_handle_outputs\n",
    "    )\n",
    "    return step_modified_mrmr_fit\n",
    "\n",
    "modified_mrmr_fit_step = make_modified_mrmr_fit_step(dcRead, s3, spark)\n",
    "modified_mrmr_fit_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba634d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('tsfresh_feature_reduction_dataframe.pickle',s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f0760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_boruta_fit_step(dcRead, s3, spark):\n",
    "    def boruta_fit_load_inputs():\n",
    "        return {\n",
    "            'split_data': spark.read.parquet(f'{s3.project_home } /observations. parquet'),\n",
    "            'tsfresh_feature_reduction_dataframe': pickle_get('tsfresh_feature_reduction_dataframe.pickle', s3)}\n",
    "\n",
    "    def boruta_fit_handle_outputs(tsfresh_feature_selection_dataframe) :\n",
    "        pickle_put(tsfresh_feature_selection_dataframe, 'tsfresh_feature_selection_dataframe.pickle', s3)\n",
    "        \n",
    "    step_boruta_fit = boruta_fit.BorutaFit(\n",
    "        input_loader = boruta_fit_load_inputs,\n",
    "        output_handler = boruta_fit_handle_outputs)\n",
    "\n",
    "    return step_boruta_fit\n",
    "\n",
    "boruta_fit_step = make_boruta_fit_step(dcRead, s3, spark)\n",
    "boruta_fit_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('tsfresh_feature_selection_dataframe.pickle',s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0032089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_select_n_features_step(\n",
    "    dcRead,\n",
    "    s3,\n",
    "    spark,\n",
    "    max_evals = MAX_EVALS,\n",
    "    n_features_to_try = N_FEATURES_TO_TRY,\n",
    "    feature_selection_method = FEATURE_SELECTION_METHOD\n",
    "    ):\n",
    "\n",
    "    def select_n_features_load_inputs():\n",
    "        return {\n",
    "        'split_data': spark.read.parquet(f'{s3.project_home} /observations.parquet'),\n",
    "        'tsfresh_feature_selection_dataframe': pickle_get('tsfresh_feature_selection_dataframe.pickle', s3)}\n",
    "\n",
    "    def select_n_features_handle_outputs (n_feats_best, n_features_tried, train_scores, val_scores, val_y_predictions):\n",
    "        pickle_put(n_feats_best, 'n_feats_best.pickle', s3)\n",
    "        pickle_put(n_features_tried, 'select_n_feats_n_feats.pickle', s3)\n",
    "        pickle_put(train_scores, 'select_n_feats_train scores.pickle', s3)\n",
    "        pickle_put(val_scores, 'select_n_feats_val_scores.pickle', s3)\n",
    "        pickle_put(val_y_predictions, 'val_y_predictions.pickle', s3)\n",
    "\n",
    "    step_select_n_features = select_n_features.SelectNFeatures(\n",
    "        parameters = {\n",
    "            'n_features_to_try': n_features_to_try,\n",
    "            'max_evals': max_evals,\n",
    "            'feature_selection method': feature_selection_method},\n",
    "        input_loader=select_n_features_load_inputs,\n",
    "        output_handler=select_n_features_handle_outputs\n",
    "    )\n",
    "    return step_select_n_features\n",
    "\n",
    "select_n_features_step = make_select_n_features_step(dcRead, s3, spark)\n",
    "select_n_features_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe53508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('n_feats_best.pickle',s3)\n",
    "#pickle_get('select_n_feats_n_feats.pickle',s3)\n",
    "#pickle_get('select_n_feats_train.pickle',s3)\n",
    "#pickle_get('select_n_feats_val_scores.pickle',s3)\n",
    "#pickle_get('val_y_predictions.pickle',s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2660a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_modified_mrmr_tune_n_features_step(dcRead, s3, spark):\n",
    "    def modified_mrmr_tune_n_features_load_inputs():\n",
    "        return {\n",
    "            'n': pickle_get('n_feats_best.pickle', s3),\n",
    "            'tsfresh_feature_selection_dataframe': pickle_get('tsfresh_feature_selection_dataframe.pickle', s3)\n",
    "        }\n",
    "    def modified_mrmr_tune_n_features_handle_outputs(feature_names) :\n",
    "        pickle_put(feature_names, 'feature_names.pickle', s3)\n",
    "\n",
    "    step_modified_mrmr_tune_n_features = modified_mrmr_tune_n_features.ModifiedMRMRTuneNFeatures(\n",
    "        input_loader = modified_mrmr_tune_n_features_load_inputs,\n",
    "        output_handler = modified_mrmr_tune_n_features_handle_outputs)\n",
    "\n",
    "    return step_modified_mrmr_tune_n_features\n",
    "\n",
    "modified_mrmr_tune_n_features_step = make_modified_mrmr_tune_n_features_step(dcRead, s3, spark)\n",
    "modified_mrmr_tune_n_features_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554ebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('feature_names.pickle',s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0aaf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_selection_transform_step(\n",
    "    dcRead,\n",
    "    s3,\n",
    "    spark,\n",
    "    non_feature_cols = NON_FEATURE_COLS):\n",
    "\n",
    "    def feature_selection_transform_load_inputs():\n",
    "        return {\n",
    "        'split_data': spark.read.parquet(f'{s3.project_home}/observations.parquet'),\n",
    "        'feature_names': pickle_get('feature_names.pickle', s3)}\n",
    "\n",
    "    def feature_selection_transform_handle_outputs(reduced_data): \n",
    "        return { \n",
    "            'reduced_data': reduced_data.write.parquet(\n",
    "            f'{s3.project_home} /reduced_data.parquet', \n",
    "            mode='overwrite')\n",
    "        }\n",
    "\n",
    "    step_mrmr_transform = feature_selection_transform.FeatureSelectionTransform(\n",
    "        parameters = {\n",
    "            'non_feature_cols': non_feature_cols},\n",
    "        input_loader=feature_selection_transform_load_inputs,\n",
    "        output_handler=feature_selection_transform_handle_outputs\n",
    "    )\n",
    "    return step_mrmr_transform\n",
    "\n",
    "feature_selection_transform_step = make_feature_selection_transform_step(dcRead, s3, spark)\n",
    "feature_selection_transform_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f69bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocess_fit_step(dcRead, s3, spark):\n",
    "    def pf_load_inputs():\n",
    "        return {\n",
    "        'split_data': spark.read.parquet (f'{s3.project_home} /reduced_data.parquet')\n",
    "        }\n",
    "    def pf_handle_outputs(scaler):\n",
    "        pickle_put (scaler, 'min_max_scaler.pickle', s3)\n",
    "\n",
    "    step_preprocess_fit = preprocess_fit.PreprocessFit(\n",
    "        input_loader = pf_load_inputs,\n",
    "        output_handler = pf_handle_outputs)\n",
    "\n",
    "    return step_preprocess_fit\n",
    "\n",
    "preprocess_fit_step = make_preprocess_fit_step(dcRead, s3, spark)\n",
    "preprocess_fit_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('min_max_scaler.pickle',s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preprocess_transform_step(dcRead, s3, spark):\n",
    "    def pt_load_inputs():\n",
    "        return {\n",
    "            'split_data': spark.read.parquet (f'{s3.project_home} /reduced_data.parquet'),\n",
    "            'scaler': pickle_get('min_max_scaler. pickle', s3)}\n",
    "\n",
    "    def pt_handle_outputs(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_val,\n",
    "        y_val,\n",
    "        X_test,\n",
    "        y_test):\n",
    "\n",
    "        pickle_put(X_train, 'X_train.pickle', s3)\n",
    "        pickle_put(y_train, 'y_train.pickle', s3)\n",
    "        pickle_put(X_val, 'x_val.pickle', s3) \n",
    "        pickle_put(y_val, 'y_val.pickle', s3)\n",
    "        pickle_put(X_test, 'X_test.pickle', s3)\n",
    "        pickle_put(y_test, 'y_test.pickle', s3)\n",
    "\n",
    "    step_preprocess_transform = preprocess_transform.PreprocessTransform(\n",
    "        input_loader=pt_load_inputs, \n",
    "        output_handler=pt_handle_outputs)\n",
    "\n",
    "    return step_preprocess_transform\n",
    "\n",
    "preprocess_transform_step = make_preprocess_transform_step(dcRead, s3, spark)\n",
    "preprocess_transform_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b119a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_get('X_train.pickle',s3)\n",
    "#pickle_get('y_train.pickle',s3)\n",
    "#pickle_get('x_val.pickle',s3)\n",
    "#pickle_get('y_val.pickle',s3)\n",
    "#pickle_get('X_test.pickle',s3)\n",
    "#pickle_get('y_test.pickle',s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cdf4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tune_train_step(dcRead, s3, spark, max_evals=MAX_EVALS_TUNE_TRAIN):\n",
    "\n",
    "    def tt_load_inputs():\n",
    "        return{\n",
    "        'X_train': pickle_get('X_train.pickle', s3),\n",
    "        'y_train': pickle_get('y_train.pickle', s3),\n",
    "        'X_val': pickle_get('X_val.pickle', s3),\n",
    "        'y_val': pickle_get('y_val.pickle', s3)}\n",
    "\n",
    "    def tt_handle_outputs (model):\n",
    "        keras_put(model, 'model.h5', s3) \n",
    "\n",
    "    step_tune_train = tune_train.TuneTrain(\n",
    "        parameters={ \n",
    "            'max_evals': max_evals \n",
    "        },\n",
    "        input_loader=tt_load_inputs, \n",
    "        output_handler=tt_handle_outputs\n",
    "    )\n",
    "    return step_tune_train\n",
    "\n",
    "tune_train_step = make_tune_train_step(dcRead, s3, spark)\n",
    "tune_train_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59281ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras_put(model, 'model.h5', s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4067015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_evaluate_step (dcRead, s3, spark):\n",
    "    def eval_load_inputs():\n",
    "        return {\n",
    "            'model': keras_get('model.h5', s3),\n",
    "            'X_test': pickle_get('X_test.pickle', s3),\n",
    "            'y_test': pickle_get('y_test.pickle', s3)}\n",
    "\n",
    "    def eval_handle_outputs (recall, precision, roc_auc):\n",
    "        print(f'Recall: {recall}')\n",
    "        print(f'Precision: {precision}')\n",
    "        print(f'ROC-AUC: {roc_auc}')\n",
    "    \n",
    "    step_evaluate = evaluate.Evaluate(\n",
    "        input_loader = eval_load_inputs,\n",
    "        output_handler = eval_handle_outputs\n",
    "    )\n",
    "    return step_evaluate\n",
    "\n",
    "evaluate_step = make_evaluate_step(dcRead, s3, spark)\n",
    "evaluate_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_compile_metadata_step(dcRead, s3, spark):\n",
    "    def cm_load_inputs():\n",
    "        return {\n",
    "            'simplified_tsfresh_map': pickle_get('full_simplified_tsfresh_map.pickle', s3),\n",
    "            'selected_features': pickle_get('feature_names.pickle', s3),\n",
    "            'scaler': pickle_get('min_max_scaler.pickle', s3)\n",
    "        }\n",
    "\n",
    "    def cm_handle_outputs (metadata):\n",
    "        json_put(metadata, 'metadata.json', s3)\n",
    "        \n",
    "    step_cm = compile_metadata.CompileMetadata(\n",
    "        input_loader=cm_load_inputs,\n",
    "        output_handler=cm_handle_outputs)\n",
    "    \n",
    "    return step_cm\n",
    "\n",
    "compile_metadata_step = make_compile_metadata_step(dcRead, s3, spark)\n",
    "compile_metadata_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ee6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_get('metadata.json', s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3554d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stage_step(dcRead, s3, spark):\n",
    "    def get_run_info() :\n",
    "        with open('/mnt/metadata/run_information.json', 'r') as f:\n",
    "            run_info = json.load(f)\n",
    "        return run_info\n",
    "\n",
    "    def stage_load_inputs():\n",
    "        return {\n",
    "            'split_data': spark.read.parquet (f'{s3.project_home} /observations.parquet'),\n",
    "            'metadata': json_get('metadata.json', s3),\n",
    "            'model': keras_get('model.h5', s3),\n",
    "            'run_info': get_run_info()\n",
    "        }\n",
    "\n",
    "    def stage_handle_outputs (split_data, metadata, model, run_info):\n",
    "        split_data.write.parquet(\n",
    "            f'{s3.project_home}/staged/split_data. parquet',\n",
    "            mode='overwrite'\n",
    "        )\n",
    "        json_put (metadata, 'staged/metadata.json', s3) \n",
    "        keras_put (model, 'staged/model.h5', s3)\n",
    "        json_put (run_info, 'staged/run_info.json', s3)\n",
    "\n",
    "    step_stage = stage.Stage(\n",
    "        input_loader = stage_load_inputs,\n",
    "        output_handler = stage_handle_outputs\n",
    "    )    \n",
    "    return step_stage\n",
    "\n",
    "\n",
    "stage_step = make_stage_step(dcRead, s3, spark)\n",
    "stage_step.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff826a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_get('staged/metadata.json', s3)\n",
    "#keras_put(model, 'model.h5', s3)\n",
    "#json_get('staged/run_info.json', s3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
